import arxiv
import os
import time
import requests
import re
import sys
import datetime
import json
from Bio import Entrez
from google import genai
from google.genai import types

# ==========================================
# 0. æ—¥å¿—ä¸é‡è¯•æœºåˆ¶ (è§£å†³ 429 æŠ¥é”™)
# ==========================================
def log(msg):
    """å°†æ—¥å¿—æ‰“å°åˆ°æ ‡å‡†é”™è¯¯æµ (stderr)"""
    print(msg, file=sys.stderr)

def safe_generate_content(client, model, contents, config=None, retries=3):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„ API è°ƒç”¨ï¼Œä¸“é—¨è§£å†³ 429 Resource Exhausted
    """
    for attempt in range(retries):
        try:
            if config:
                response = client.models.generate_content(
                    model=model, contents=contents, config=config
                )
            else:
                response = client.models.generate_content(
                    model=model, contents=contents
                )
            return response
        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                wait_time = 30 * (attempt + 1) # ç¬¬ä¸€æ¬¡ç­‰30ç§’ï¼Œç¬¬äºŒæ¬¡ç­‰60ç§’
                log(f"âš ï¸ è§¦å‘é™æµ (429)ï¼Œä¼‘æ¯ {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                log(f"âŒ API è°ƒç”¨é”™è¯¯: {e}")
                return None
    return None

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸é‰´æƒ
# ==========================================
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
ENTREZ_EMAIL = "dongwei_li@hotmail.com" # [å·²ä¿ç•™]

if not GOOGLE_API_KEY:
    raise ValueError("âŒ æœªæ‰¾åˆ° GOOGLE_API_KEY")

if "@" not in ENTREZ_EMAIL:
    log("âŒ é”™è¯¯ï¼šé‚®ç®±æ ¼å¼ä¸æ­£ç¡®ï¼")
    sys.exit(1)

Entrez.email = ENTREZ_EMAIL

client = genai.Client(api_key=GOOGLE_API_KEY)
MODEL_NAME = "gemini-2.5-flash"

# ==========================================
# 2. æœç´¢ç­–ç•¥
# ==========================================
SEARCH_KEYWORDS = [
    "single-cell", "scRNA-seq", "spatial transcriptomics", "chromatin accessibility",
    "foundation model", "transformer", "deep learning genomics",
    "plant", "Arabidopsis", "rice", "maize", "crop breeding"
]
COMPILED_PATTERNS = [re.compile(rf'\b{re.escape(k.lower())}\b') for k in SEARCH_KEYWORDS]

# ==========================================
# 3. Prompt: é˜¶æ®µä¸€ (è£åˆ¤ - è¯„åˆ†ä¸åˆ†ç±»)
# ==========================================
RELEVANCE_PROMPT_TEMPLATE = """
You are a domain expert in **Plant single-cell biology** and **AI-driven crop breeding**.
Your task is to JUDGE the relevance of this paper.

Title: {title}
Abstract: {abstract}

Step 1: Relevance Scoring (0-3)
- Plant relevance (0: None, 3: Core plant study)
- Single-cell/Omics relevance (0: None, 3: Core single-cell/spatial)
- AI/Modeling relevance (0: None, 3: Deep learning/Foundation model)
- Breeding relevance (0: None, 3: Trait prediction/Improvement)

Step 2: Extract Species
- Extract the main organism/species studied (e.g., "Rice (Oryza sativa)", "Arabidopsis", "Human", "General Model").

Step 3: Decision
- KEEP: Highly relevant.
- DROP: Totally irrelevant.

Step 4: Tagging
- ATLAS, METHOD, APPLICATION, BREEDING

Output JSON format only:
{{
  "plant_score": int,
  "single_cell_score": int,
  "ai_score": int,
  "breeding_score": int,
  "species": "String",
  "decision": "KEEP" | "DROP",
  "tags": ["TAG1", "TAG2"],
  "reason": "Short reason"
}}
"""

# ==========================================
# 4. Prompt: é˜¶æ®µäºŒ (å‚è°‹ - æ·±åº¦ç ”è¯»)
# ==========================================
DEEP_DIVE_PROMPT_TEMPLATE = """
# Role
ä½ æ˜¯æˆ‘ï¼ˆæ¤ç‰©å•ç»†èƒ+AIè‚²ç§åšå£«ï¼‰çš„**ç§‘ç ”å‚è°‹**ã€‚
è¿™ç¯‡è®ºæ–‡å·²è¢«åˆ¤å®šä¸º**é«˜ä»·å€¼**ã€‚è¯·è¿›è¡Œé»‘å®¢å¼æ‹†è§£ã€‚

# Metadata
Title: {title}
Species: {species}
Tags: {tags}

# Output Requirements (Strict Markdown)
## ğŸ“‘ [ä¸­æ–‡æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{title}
**æ¥æº**ï¼š{source} | **å‘å¸ƒæ—¶é—´**ï¼š{date}
**ç ”ç©¶ç‰©ç§**ï¼š`{species}` | **æ ‡ç­¾**ï¼š`{tags}`

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[åœ¨æ­¤å¤„æ’°å†™ 150-200 å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚ä¸»è¦æè¿°è®ºæ–‡çš„èƒŒæ™¯é—®é¢˜ã€æå‡ºçš„æ–¹æ³•è®ºä»¥åŠæœ€ç»ˆè¾¾æˆçš„æ•ˆæœã€‚]

### ğŸ§  ç ”ç©¶æ€è·¯å¤ç›˜ (The Logic Chain)
*ä¸è¦åªå‘Šè¯‰æˆ‘ä»–åšäº†ä»€ä¹ˆï¼Œè¦å‘Šè¯‰æˆ‘ä»–æ˜¯æ€ä¹ˆæƒ³åˆ°çš„ã€‚*
* **ğŸ” ç ´å±€ç‚¹ (The Spark)**ï¼šä½œè€…æ˜¯çœ‹åˆ°äº†ä»€ä¹ˆç—›ç‚¹ï¼Œæ‰æƒ³å‡ºäº†è¿™ä¸ªæ–¹æ³•çš„ï¼Ÿ
* **ğŸ› ï¸ æŠ€æœ¯é€‰å‹é€»è¾‘**ï¼šä¸ºä»€ä¹ˆä»–é€‰äº† A æ–¹æ³•è€Œä¸æ˜¯ B æ–¹æ³•ï¼Ÿ
* **â›“ï¸ å®éªŒè®¾è®¡é—­ç¯**ï¼šä»–æ˜¯æ€ä¹ˆè¯æ˜è‡ªå·±æ˜¯å¯¹çš„ï¼Ÿ

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* **[åˆ›æ–°ç‚¹ 1 - æŠ€æœ¯åŸç†]**ï¼šè¯¦ç»†è§£é‡Šè¯¥åˆ›æ–°çš„æŠ€æœ¯åŸç†æˆ–å®ç°æ–¹å¼ï¼Œä»¥åŠå®ƒç›¸å¯¹äºç°æœ‰ SOTA æ–¹æ³•çš„ä¼˜åŠ¿ã€‚
* **[åˆ›æ–°ç‚¹ 2 - å®éªŒè®¾è®¡]**ï¼šæè¿°è¯¥æ–¹æ³•åœ¨å®éªŒè®¾è®¡æˆ–æ•°æ®é›†æ„å»ºä¸Šçš„ç‹¬ç‰¹ä¹‹å¤„ã€‚
* **[åˆ›æ–°ç‚¹ 3 - é‡åŒ–çªç ´]**ï¼šæ€»ç»“è¯¥è®ºæ–‡åœ¨å®éªŒç»“æœä¸Šçš„çªç ´ï¼ˆéœ€åŒ…å«å…·ä½“çš„æå‡æ•°æ®ï¼Œå¦‚ Accuracy æå‡äº† x%ï¼‰ã€‚

### ğŸ™‹â€â™‚ï¸ å¯¹æˆ‘ï¼ˆæ¤ç‰©/å•ç»†èƒï¼‰çš„å€Ÿé‰´ (Actionable Insights)
* **è¿ç§»æ½œåŠ›**ï¼š
    * *å¦‚æœæ˜¯äººç±»/åŠ¨ç‰©ç ”ç©¶*ï¼šè¿™ä¸ªæ€è·¯èƒ½ç›´æ¥å¥—ç”¨åˆ°**æ°´ç¨»/æ‹Ÿå—èŠ¥**ä¸Šå—ï¼Ÿéœ€è¦æ”¹ä»€ä¹ˆï¼Ÿ
    * *å¦‚æœæ˜¯AIç®—æ³•*ï¼šè¿™ä¸ªæ¨¡å‹æ¶æ„é€‚åˆå¤„ç†**æ¤ç‰©åŸºå› ç»„çš„å¤šå€ä½“/é«˜é‡å¤åºåˆ—**ç‰¹å¾å—ï¼Ÿ

### ğŸ“‰ é¿å‘æŒ‡å—
* æ•°æ®è¦æ±‚é«˜å—ï¼Ÿæ˜¾å­˜å ç”¨å¤§å—ï¼Ÿä»£ç å¼€æºäº†å—ï¼Ÿ

---
# Input Abstract
{abstract}
"""

# ==========================================
# 5. å·¥å…·å‡½æ•°
# ==========================================
def parse_pubmed_abstract(article_data):
    abstract_obj = article_data.get('Abstract', {}).get('AbstractText', [])
    if not abstract_obj: return "No Abstract"
    parts = []
    items = abstract_obj if isinstance(abstract_obj, list) else [abstract_obj]
    for item in items:
        if isinstance(item, str): parts.append(item)
        elif isinstance(item, dict):
            text = item.get('#text') or item.get('content') or ""
            label = item.get('Label', '')
            parts.append(f"**{label}**: {text}" if label else text)
    return " ".join(parts)

def is_duplicate(seen_set, title, source):
    key = (title.lower().strip(), source)
    if key in seen_set: return True
    seen_set.add(key)
    return False

# ==========================================
# 6. æ ¸å¿ƒé€»è¾‘ï¼šAI è£åˆ¤ (Judge)
# ==========================================
def evaluate_paper_relevance(paper):
    """è°ƒç”¨ Gemini åˆ¤æ–­è®ºæ–‡æ˜¯å¦å€¼å¾—è¯»ï¼Œè¿”å› JSON"""
    prompt = RELEVANCE_PROMPT_TEMPLATE.format(
        title=paper['title'],
        abstract=paper['abstract']
    )
    # ä½¿ç”¨å¸¦é‡è¯•çš„å®‰å…¨è°ƒç”¨
    response = safe_generate_content(
        client, 
        MODEL_NAME, 
        prompt, 
        config=types.GenerateContentConfig(response_mime_type="application/json")
    )
    
    if response and response.text:
        try:
            return json.loads(response.text)
        except:
            return {"decision": "KEEP", "tags": ["PARSE_ERROR"], "species": "Unknown", "reason": "JSON Error"}
    return {"decision": "DROP", "tags": [], "reason": "API Error"}

# ==========================================
# 7. æ ¸å¿ƒé€»è¾‘ï¼šAI å‚è°‹ (Analyst)
# ==========================================
def generate_deep_dive(paper, evaluation):
    """å¯¹é«˜åˆ†è®ºæ–‡è¿›è¡Œæ·±åº¦è§£è¯»"""
    transfer_hint = "å¦‚æœæ˜¯äººç±»ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æå¦‚ä½•è¿ç§»åˆ°æ¤ç‰©ç»†èƒå£/å¤šå€ä½“åœºæ™¯ã€‚"
    if "METHOD" in evaluation['tags']:
        transfer_hint += " é‡ç‚¹å…³æ³¨ç®—æ³•æ˜¯å¦èƒ½å¤„ç†æ¤ç‰©æ•°æ®çš„ç¨€ç–æ€§ã€‚"

    prompt = DEEP_DIVE_PROMPT_TEMPLATE.format(
        title=paper['title'],
        source=paper['source'],
        date=paper['date'],
        tags=",".join(evaluation['tags']),
        species=evaluation.get('species', 'N/A'),
        transfer_hint=transfer_hint,
        abstract=paper['abstract']
    )
    
    # ä½¿ç”¨å¸¦é‡è¯•çš„å®‰å…¨è°ƒç”¨
    response = safe_generate_content(client, MODEL_NAME, prompt)
    
    if response and response.text:
        return response.text
    return f"> âŒ è§£è¯»å¤±è´¥ï¼šAPIå¤šæ¬¡é‡è¯•åæ— å“åº”ã€‚"

# ==========================================
# 8. æŠ“å–å‡½æ•°
# ==========================================
def fetch_arxiv(seen_set, max_results=10):
    log("ğŸ“¡ [ArXiv] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    query = ' OR '.join([f'ti:"{k}"' for k in SEARCH_KEYWORDS[:5]]) + \
            ' OR ' + ' OR '.join([f'abs:"{k}"' for k in SEARCH_KEYWORDS[:5]])
    client_arxiv = arxiv.Client(page_size=max_results, delay_seconds=3, num_retries=3)
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    try:
        for result in client_arxiv.results(search):
            if is_duplicate(seen_set, result.title, "ArXiv"): continue
            papers.append({
                "title": result.title, "abstract": result.summary,
                "url": result.entry_id, "date": result.published.strftime("%Y-%m-%d"),
                "source": "ArXiv"
            })
    except Exception as e: log(f"âš ï¸ ArXiv Error: {e}")
    return papers

def fetch_biorxiv(seen_set, limit=10):
    log("ğŸ“¡ [BioRxiv] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    try:
        today = datetime.date.today()
        from_date = today - datetime.timedelta(days=7)
        cursor = "0"
        total_fetched = 0
        while True:
            url = f"https://api.biorxiv.org/details/biorxiv/{from_date}/{today}/{cursor}/json"
            resp = requests.get(url).json()
            collection = resp.get('collection', [])
            messages = resp.get('messages', [{}])[0]
            if not collection: break
            for item in collection:
                if total_fetched >= limit: break
                title = item['title']
                if is_duplicate(seen_set, title, "BioRxiv"): continue
                text_check = (title + item['abstract']).lower()
                if any(k.lower() in text_check for k in SEARCH_KEYWORDS):
                    papers.append({
                        "title": title, "abstract": item['abstract'],
                        "url": f"https://doi.org/{item['doi']}", "date": item['date'],
                        "source": "BioRxiv"
                    })
                    total_fetched += 1
            new_cursor = messages.get('next-cursor')
            if not new_cursor or str(new_cursor) == str(cursor) or total_fetched >= limit: break
            cursor = str(new_cursor)
            time.sleep(1)
    except Exception as e: log(f"âš ï¸ BioRxiv Error: {e}")
    return papers

def fetch_pubmed(seen_set, max_results=5):
    log("ğŸ“¡ [PubMed] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    today_str = datetime.date.today().strftime("%Y/%m/%d")
    past_str = (datetime.date.today() - datetime.timedelta(days=7)).strftime("%Y/%m/%d")
    date_term = f' AND ("{past_str}"[PDAT] : "{today_str}"[PDAT])'
    term = ' OR '.join([f'({k})' for k in SEARCH_KEYWORDS]) + date_term
    try:
        handle = Entrez.esearch(db="pubmed", term=term, retmax=max_results, sort="date")
        record = Entrez.read(handle)
        id_list = record["IdList"]
        handle.close()
        if not id_list: return []
        time.sleep(2)
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="abstract", retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        for article in records['PubmedArticle']:
            try:
                article_data = article['MedlineCitation']['Article']
                title = article_data['ArticleTitle']
                if is_duplicate(seen_set, title, "PubMed"): continue
                papers.append({
                    "title": title, "abstract": parse_pubmed_abstract(article_data),
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{article['MedlineCitation']['PMID']}/",
                    "date": today_str, "source": "PubMed"
                })
            except: continue
    except Exception as e: log(f"âš ï¸ PubMed Error: {e}")
    return papers

# ==========================================
# 9. ä¸»æµç¨‹ (é€»è¾‘æ›´æ–°ï¼šå…ˆè¯„åˆ†ï¼Œå†æ’åºï¼Œåç ”è¯»)
# ==========================================
def process_papers(papers):
    
    # 1. è¯„åˆ†é˜¶æ®µ (Phase 1: Judging)
    log(f"âš–ï¸ å¼€å§‹ç¬¬ä¸€è½®ç­›é€‰ (å…± {len(papers)} ç¯‡)...")
    kept_papers = []
    
    for paper in papers:
        eval_result = evaluate_paper_relevance(paper)
        
        # è°ƒè¯•è¾“å‡º
        decision = eval_result.get('decision', 'DROP')
        species = eval_result.get('species', 'N/A')
        log(f"   -> {paper['title'][:20]}... | {decision} | {species}")
        
        if decision == "KEEP":
            paper['eval'] = eval_result # æŠŠè¯„åˆ†ç»“æœå­˜è¿›å»
            kept_papers.append(paper)
        
        # å³ä½¿æ˜¯è¯„åˆ†ï¼Œä¹ŸåŠ ä¸€ç‚¹å»¶è¿Ÿé˜²æ­¢ 429
        time.sleep(2)

    if not kept_papers:
        return "", 0

    # 2. æ’åºé˜¶æ®µ (Phase 2: Sorting)
    # æ’åºé€»è¾‘ï¼š
    # Group 1: Plant Score >= 2 (æ¤ç‰©ç›¸å…³ï¼Œæ”¾æœ€å‰)
    # Group 2: AI Score >= 2 (æ–¹æ³•ç›¸å…³ï¼Œæ”¾ä¸­é—´)
    # Group 3: Others (å…¶ä»–è¿ç§»ï¼Œæ”¾æœ€å)
    log("ğŸ”„ æ­£åœ¨æ™ºèƒ½æ’åº...")
    
    def sort_key(p):
        plant_score = p['eval'].get('plant_score', 0)
        ai_score = p['eval'].get('ai_score', 0)
        
        # è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒPythonä¼šæŒ‰é¡ºåºæ¯”è¾ƒ
        # è´Ÿå·æ˜¯å› ä¸ºè¦é™åºæ’åˆ— (åˆ†æ•°é«˜çš„åœ¨å‰)
        if plant_score >= 2:
            return (0, -plant_score, -ai_score) # ä¼˜å…ˆçº§ 0 (æœ€é«˜)
        elif ai_score >= 2:
            return (1, -ai_score, -plant_score) # ä¼˜å…ˆçº§ 1
        else:
            return (2, -ai_score, -plant_score) # ä¼˜å…ˆçº§ 2
            
    kept_papers.sort(key=sort_key)

    # 3. ç ”è¯»é˜¶æ®µ (Phase 3: Deep Dive)
    log(f"ğŸ§  å¼€å§‹æ·±åº¦ç ”è¯» (å…¥é€‰ {len(kept_papers)} ç¯‡)...")
    report_content = ""
    
    for paper in kept_papers:
        summary = generate_deep_dive(paper, paper['eval'])
        
        report_content += summary
        report_content += f"\nğŸ”— **åŸæ–‡ç›´è¾¾**: [{paper['source']} Link]({paper['url']})\n"
        
        # æ·»åŠ åº•éƒ¨çŠ¶æ€æ 
        tags = paper['eval'].get('tags', [])
        plant_score = paper['eval'].get('plant_score', 0)
        ai_score = paper['eval'].get('ai_score', 0)
        report_content += f"> ğŸ·ï¸ **è‡ªåŠ¨æ ‡ç­¾**: `{', '.join(tags)}` | ğŸ“Š **è¯„åˆ†**: Plant({plant_score}) AI({ai_score})\n"
        report_content += "---\n\n"
        
        # ç ”è¯»åå¿…é¡»sleepï¼Œé˜²æ­¢ Deep Dive è§¦å‘é™æµ
        time.sleep(5) 

    return report_content, len(kept_papers)

def main():
    log(f"ğŸš€ å¯åŠ¨ Bio-AI æƒ…æŠ¥ Agent (v12.0 Sorted & Retry)...")
    seen_papers = set()
    all_papers = []
    
    all_papers.extend(fetch_arxiv(seen_papers, max_results=10))
    all_papers.extend(fetch_biorxiv(seen_papers, limit=10))
    all_papers.extend(fetch_pubmed(seen_papers, max_results=5))
    
    log(f"\nğŸ“Š å®½å¬å›é˜¶æ®µï¼šå…±è·å– {len(all_papers)} ç¯‡å€™é€‰è®ºæ–‡...\n")
    
    if not all_papers:
        log("æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ã€‚")
        return

    # å¤„ç†æµç¨‹ (åŒ…å«è¯„åˆ†ã€æ’åºã€ç ”è¯»)
    report_body, kept_count = process_papers(all_papers)

    # ç”ŸæˆæŠ¥å‘Šå¤´
    daily_report = f"# ğŸ§  Bio-AI æ¯æ—¥æƒ…æŠ¥å†³ç­– ({datetime.date.today()})\n"
    daily_report += f"> ğŸ“Š ä»Šæ—¥å¤§ç›˜ï¼šå¬å› {len(all_papers)} ç¯‡ -> AI ä¸¥é€‰ {kept_count} ç¯‡\n"
    daily_report += "> ğŸ¤– æ’åºç­–ç•¥ï¼šæ¤ç‰©ç ”ç©¶ > æ ¸å¿ƒç®—æ³• > è¿ç§»å€Ÿé‰´\n\n"
    
    if kept_count == 0:
        daily_report += "### ä»Šæ—¥æ— é«˜ä»·å€¼è®ºæ–‡å…¥é€‰\nå»ºè®®æ˜å¤©ç»§ç»­å…³æ³¨ã€‚\n"
    else:
        daily_report += report_body

    print(daily_report)
    log("\nâœ… ä»»åŠ¡å®Œæˆã€‚")

if __name__ == "__main__":
    main()
