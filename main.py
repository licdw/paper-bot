import arxiv
import os
import time
import requests
import re
import sys
import datetime
import json
import random
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from Bio import Entrez
from google import genai
from google.genai import types

# ==========================================
# 0. é…ç½®ä¸æ—¥å¿—
# ==========================================
def log(msg):
    print(msg, file=sys.stderr)

# API Keys & Config
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
ZHIPU_API_KEY = os.getenv('ZHIPU_API_KEY')
EMAIL_USER = "dongwei_li@hotmail.com" 
EMAIL_PASSWORD = os.getenv('EMAIL_PASSWORD')
EMAIL_TO = "dongwei_li@hotmail.com"

# [æ ¸å¿ƒä¿®æ”¹] ä¸¥æ ¼çš„å†·å´æ—¶é—´é…ç½® (ç§’)
GEMINI_COOLDOWN = 300  # Gemini ä¼‘æ¯ 5 åˆ†é’Ÿ
ZHIPU_COOLDOWN = 180   # æ™ºè°± ä¼‘æ¯ 3 åˆ†é’Ÿ

if not GOOGLE_API_KEY: raise ValueError("âŒ æœªæ‰¾åˆ° GOOGLE_API_KEY")
Entrez.email = EMAIL_USER

# åˆå§‹åŒ–
client_gemini = genai.Client(api_key=GOOGLE_API_KEY)
GEMINI_MODEL = "gemini-2.5-flash"

# ==========================================
# 1. é‚®ä»¶å‘é€æ¨¡å—
# ==========================================
def send_email(subject, body_markdown):
    if not EMAIL_PASSWORD:
        log("âš ï¸ æœªé…ç½® EMAIL_PASSWORDï¼Œè·³è¿‡é‚®ä»¶å‘é€ã€‚")
        return
    msg = MIMEMultipart()
    msg['From'] = EMAIL_USER
    msg['To'] = EMAIL_TO
    msg['Subject'] = subject
    msg.attach(MIMEText(body_markdown, 'plain', 'utf-8'))
    try:
        server = smtplib.SMTP('smtp.office365.com', 587)
        server.starttls()
        server.login(EMAIL_USER, EMAIL_PASSWORD)
        server.sendmail(EMAIL_USER, EMAIL_TO, msg.as_string())
        server.quit()
        log(f"âœ… é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {EMAIL_TO}")
    except Exception as e:
        log(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")

# ==========================================
# 2. æ™ºèƒ½ç”Ÿæˆæ¨¡å— (ä¸»åŠ¨äº¤æ›¿ + ä¸¥æ ¼é™æµ)
# ==========================================
def call_gemini(prompt, is_json=False):
    """åº•å±‚ï¼šè°ƒç”¨ Gemini"""
    try:
        config = types.GenerateContentConfig(response_mime_type="application/json") if is_json else None
        response = client_gemini.models.generate_content(model=GEMINI_MODEL, contents=prompt, config=config)
        return response.text
    except Exception as e: raise e

def call_zhipu(prompt, is_json=False):
    """åº•å±‚ï¼šè°ƒç”¨æ™ºè°± GLM-4"""
    if not ZHIPU_API_KEY: raise Exception("No Zhipu Key Configured")
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç”Ÿç‰©ä¿¡æ¯å­¦ç§‘ç ”åŠ©æ‰‹ã€‚"}, {"role": "user", "content": prompt}]
    if is_json: messages[0]["content"] += " è¯·è¾“å‡ºä¸¥æ ¼JSONã€‚"
    payload = {"model": "glm-4-flash", "messages": messages, "stream": False, "temperature": 0.5}
    headers = {"Authorization": f"Bearer {ZHIPU_API_KEY}", "Content-Type": "application/json"}
    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=60)
        return resp.json()['choices'][0]['message']['content'].replace("```json", "").replace("```", "").strip()
    except Exception as e: raise e

def generate_with_strategy(prompt, preferred_engine="gemini", is_json=False):
    """
    [æ ¸å¿ƒé€»è¾‘] æ ¹æ®æŒ‡å®šçš„é¦–é€‰å¼•æ“å°è¯•ç”Ÿæˆ
    è¿”å›: (content, used_engine)
    """
    # 1. å°è¯•é¦–é€‰å¼•æ“
    if preferred_engine == "gemini":
        try:
            return call_gemini(prompt, is_json), "gemini"
        except Exception as e:
            log(f"   âš ï¸ Gemini å¤±è´¥ ({e})ï¼Œå°è¯•åˆ‡æ¢æ™ºè°±...")
            # å¤±è´¥åˆ™å›é€€åˆ°æ™ºè°±
            try:
                if ZHIPU_API_KEY: return call_zhipu(prompt, is_json), "zhipu"
            except: pass
            
    elif preferred_engine == "zhipu":
        try:
            if ZHIPU_API_KEY: return call_zhipu(prompt, is_json), "zhipu"
            else: raise Exception("No Zhipu Key")
        except Exception as e:
            log(f"   âš ï¸ æ™ºè°± å¤±è´¥ ({e})ï¼Œå°è¯•åˆ‡æ¢ Gemini...")
            # å¤±è´¥åˆ™å›é€€åˆ° Gemini
            try:
                return call_gemini(prompt, is_json), "gemini"
            except: pass

    return None, "none"

# ==========================================
# 2. æœç´¢ç­–ç•¥ï¼šå®½å¬å› (Broad Recall)
# ==========================================
# ç­–ç•¥ï¼šç”¨æœ€å°‘çš„è¯ï¼Œè¦†ç›–æœ€å¤§çš„é¢ã€‚ä¸è¦å¤ªç»†ï¼Œå¤ªç»†äº†ä¼šæ¼ã€‚
SEARCH_KEYWORDS = [
    # --- æ–¹å‘1: æ¤ç‰©å•ç»†èƒ & å›¾è°± ---
    "plant single-cell", "scRNA-seq", "spatial transcriptomics", "cell atlas",
    
    # --- æ–¹å‘2: æ•°æ®æ•´åˆ & å¤šç»„å­¦ ---
    "data integration", "multi-omics", "reference mapping",
    
    # --- æ–¹å‘3: AIè‚²ç§ & åŸºç¡€æ¨¡å‹ ---
    "foundation model", "deep learning genomics", "AI breeding", "trait prediction",
    
    # --- æ ¸å¿ƒç‰©ç§é™åˆ¶ (è¾…åŠ©) ---
    "plant", "Arabidopsis", "rice", "maize" 
]

# é¢„ç¼–è¯‘æ­£åˆ™ (ä¿æŒä¸å˜)
COMPILED_PATTERNS = [re.compile(rf'\b{re.escape(k.lower())}\b') for k in SEARCH_KEYWORDS]

# ==========================================
# 3. Prompt: é˜¶æ®µä¸€ (è£åˆ¤ - è¯„åˆ†ä¸åˆ†ç±»)
# ==========================================
RELEVANCE_PROMPT_TEMPLATE = """
You are a domain expert in **Plant Single-Cell**, **Data Integration**, and **AI Breeding**.
Your task is to JUDGE the relevance of this paper based on the user's specific research interests.

User's Core Interests:
1. **Plant Single-Cell**: scRNA-seq atlas, spatial transcriptomics, developmental trajectory.
2. **Data Integration**: Cross-species/dataset integration, batch correction, reference mapping, foundation models for representation learning.
3. **Plant AI Breeding**: Genotype-to-phenotype prediction, regulatory variant effect, crop improvement using AI.

Paper Metadata:
Title: {title}
Abstract: {abstract}

Step 1: Relevance Scoring (0-3) for EACH dimension:
- **Plant/Crop Relevance**: (0=None, 1=General Bio, 2=Plant Related, 3=Core Plant/Crop Study)
- **Single-Cell/Omics Relevance**: (0=None, 1=Bulk, 2=Single-Cell/Spatial/Multi-omics, 3=Atlas/Integration Level)
- **AI/Algorithm Relevance**: (0=None, 1=Stats, 2=ML/DL Application, 3=Foundation Model/New Algorithm)
- **Breeding/Function Relevance**: (0=None, 1=Basic Bio, 2=Functional study, 3=Breeding/Trait Prediction)

Step 2: Extract Species
- Extract the main organism (e.g., "Rice", "Maize", "Arabidopsis", "General Method").

Step 3: Decision Logic (Strict)
- **KEEP**: If the paper matches AT LEAST ONE of the User's Core Interests strongly (Score >= 2 in relevant dimensions).
    - Example: A generic AI method for single-cell integration is KEEP (transferable).
    - Example: A pure clinical human study is DROP.
- **DROP**: If strictly irrelevant (e.g., human cancer drug trials, pure math without bio application).

Step 4: Auto-Tagging
- Select tags: [ATLAS], [INTEGRATION], [AI_BREEDING], [METHOD], [SPATIAL], [MULTI_OMICS]

Output JSON format only:
{{
  "plant_score": int,
  "single_cell_score": int,
  "ai_score": int,
  "breeding_score": int,
  "species": "String",
  "decision": "KEEP" | "DROP",
  "tags": ["TAG1", "TAG2"],
  "reason": "One short sentence explaining why it matches the user's interests."
}}
"""

# ==========================================
# 4. Prompt: é˜¶æ®µäºŒ (å‚è°‹ - æ·±åº¦ç ”è¯»)
# ==========================================
DEEP_DIVE_PROMPT_TEMPLATE = """
# Role
ä½ æ˜¯æˆ‘ï¼ˆæ¤ç‰©å•ç»†èƒ+AIè‚²ç§åšå£«ï¼‰çš„**ç§‘ç ”å‚è°‹**ã€‚
è¿™ç¯‡è®ºæ–‡å·²è¢«åˆ¤å®šä¸º**é«˜ä»·å€¼**ã€‚è¯·è¿›è¡Œé»‘å®¢å¼æ‹†è§£ã€‚

# Metadata
Title: {title}
Species: {species}
Tags: {tags}

# Output Requirements (Strict Markdown)
## ğŸ“‘ [ä¸­æ–‡æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{title}
**æ¥æº**ï¼š{source} | **å‘å¸ƒæ—¶é—´**ï¼š{date}
**ç ”ç©¶ç‰©ç§**ï¼š`{species}` | **æ ‡ç­¾**ï¼š`{tags}`

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[åœ¨æ­¤å¤„æ’°å†™ 150-200 å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚ä¸»è¦æè¿°è®ºæ–‡çš„èƒŒæ™¯é—®é¢˜ã€æå‡ºçš„æ–¹æ³•è®ºä»¥åŠæœ€ç»ˆè¾¾æˆçš„æ•ˆæœã€‚]

### ğŸ§  ç ”ç©¶æ€è·¯å¤ç›˜ (The Logic Chain)
*ä¸è¦åªå‘Šè¯‰æˆ‘ä»–åšäº†ä»€ä¹ˆï¼Œè¦å‘Šè¯‰æˆ‘ä»–æ˜¯æ€ä¹ˆæƒ³åˆ°çš„ã€‚*
* **ğŸ” ç ´å±€ç‚¹ (The Spark)**ï¼šä½œè€…æ˜¯çœ‹åˆ°äº†ä»€ä¹ˆç—›ç‚¹ï¼Œæ‰æƒ³å‡ºäº†è¿™ä¸ªæ–¹æ³•çš„ï¼Ÿ
* **ğŸ› ï¸ æŠ€æœ¯é€‰å‹é€»è¾‘**ï¼šä¸ºä»€ä¹ˆä»–é€‰äº† A æ–¹æ³•è€Œä¸æ˜¯ B æ–¹æ³•ï¼Ÿ
* **â›“ï¸ å®éªŒè®¾è®¡é—­ç¯**ï¼šä»–æ˜¯æ€ä¹ˆè¯æ˜è‡ªå·±æ˜¯å¯¹çš„ï¼Ÿ

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* **[åˆ›æ–°ç‚¹ 1 - æŠ€æœ¯åŸç†]**ï¼šè¯¦ç»†è§£é‡Šè¯¥åˆ›æ–°çš„æŠ€æœ¯åŸç†æˆ–å®ç°æ–¹å¼ï¼Œä»¥åŠå®ƒç›¸å¯¹äºç°æœ‰ SOTA æ–¹æ³•çš„ä¼˜åŠ¿ã€‚
* **[åˆ›æ–°ç‚¹ 2 - å®éªŒè®¾è®¡]**ï¼šæè¿°è¯¥æ–¹æ³•åœ¨å®éªŒè®¾è®¡æˆ–æ•°æ®é›†æ„å»ºä¸Šçš„ç‹¬ç‰¹ä¹‹å¤„ã€‚
* **[åˆ›æ–°ç‚¹ 3 - é‡åŒ–çªç ´]**ï¼šæ€»ç»“è¯¥è®ºæ–‡åœ¨å®éªŒç»“æœä¸Šçš„çªç ´ï¼ˆéœ€åŒ…å«å…·ä½“çš„æå‡æ•°æ®ï¼Œå¦‚ Accuracy æå‡äº† x%ï¼‰ã€‚

### ğŸ™‹â€â™‚ï¸ å¯¹æˆ‘ï¼ˆæ¤ç‰©/å•ç»†èƒï¼‰çš„å€Ÿé‰´ (Actionable Insights)
* **è¿ç§»æ½œåŠ›**ï¼š
    * *å¦‚æœæ˜¯äººç±»/åŠ¨ç‰©ç ”ç©¶*ï¼šè¿™ä¸ªæ€è·¯èƒ½ç›´æ¥å¥—ç”¨åˆ°**æ°´ç¨»/æ‹Ÿå—èŠ¥**ä¸Šå—ï¼Ÿéœ€è¦æ”¹ä»€ä¹ˆï¼Ÿ
    * *å¦‚æœæ˜¯AIç®—æ³•*ï¼šè¿™ä¸ªæ¨¡å‹æ¶æ„é€‚åˆå¤„ç†**æ¤ç‰©åŸºå› ç»„çš„å¤šå€ä½“/é«˜é‡å¤åºåˆ—**ç‰¹å¾å—ï¼Ÿ

### ğŸ“‰ é¿å‘æŒ‡å—
* æ•°æ®è¦æ±‚é«˜å—ï¼Ÿæ˜¾å­˜å ç”¨å¤§å—ï¼Ÿä»£ç å¼€æºäº†å—ï¼Ÿ

---
# Input Abstract
{abstract}
"""

# ==========================================
# 5. å·¥å…·å‡½æ•°
# ==========================================
def parse_pubmed_abstract(article_data):
    try:
        abstract_obj = article_data.get('Abstract', {}).get('AbstractText', [])
        if not abstract_obj: return "No Abstract"
        parts = []
        items = abstract_obj if isinstance(abstract_obj, list) else [abstract_obj]
        for item in items:
            if isinstance(item, str): parts.append(item)
            elif isinstance(item, dict): parts.append(item.get('#text') or "")
        return " ".join(parts)
    except: return "No Abstract"

def is_duplicate(seen_set, title, source):
    key = (title.lower().strip(), source)
    if key in seen_set: return True
    seen_set.add(key)
    return False

# ==========================================
# 6. æ ¸å¿ƒæµç¨‹ (ä¸¥æ ¼æ…¢é€Ÿäº¤æ›¿)
# ==========================================
def process_papers(papers):
    log(f"âš–ï¸ å¼€å§‹ç­›é€‰ {len(papers)} ç¯‡è®ºæ–‡ (æ…¢é€Ÿäº¤æ›¿æ¨¡å¼)...")
    kept_papers = []
    
    # å¼•æ“åˆ‡æ¢å¼€å…³: 0=Gemini, 1=Zhipu
    engine_toggle = 0 
    
    # --- Phase 1: è¯„åˆ†ç­›é€‰ ---
    for i, paper in enumerate(papers):
        # å†³å®šå½“å‰ç”¨å“ªä¸ªå¼•æ“
        current_engine = "gemini" if (engine_toggle % 2 == 0) else "zhipu"
        
        log(f"   [{i+1}/{len(papers)}] æ­£åœ¨è¯„åˆ† (å¼•æ“: {current_engine})...")
        
        prompt = RELEVANCE_PROMPT_TEMPLATE.format(title=paper['title'], abstract=paper['abstract'])
        
        # æ‰§è¡Œè°ƒç”¨
        resp, used_engine = generate_with_strategy(prompt, preferred_engine=current_engine, is_json=True)
        
        # å¤„ç†ç»“æœ
        try:
            eval_result = json.loads(resp)
        except:
            eval_result = {"decision": "KEEP", "tags": ["ERROR"], "species": "N/A"} # å®¹é”™
            
        if eval_result.get('decision') == "KEEP":
            paper['eval'] = eval_result
            kept_papers.append(paper)
            log(f"     -> âœ… KEEP")
        else:
            log(f"     -> â­ï¸ DROP")
            
        # [å…³é”®] æ ¹æ®å®é™…ä½¿ç”¨çš„å¼•æ“ï¼Œæ‰§è¡Œä¸¥æ ¼å†·å´
        if used_engine == "gemini":
            log(f"     â³ Gemini å®Œæˆï¼Œä¼‘æ¯ {GEMINI_COOLDOWN} ç§’...")
            time.sleep(GEMINI_COOLDOWN)
        elif used_engine == "zhipu":
            log(f"     â³ æ™ºè°± å®Œæˆï¼Œä¼‘æ¯ {ZHIPU_COOLDOWN} ç§’...")
            time.sleep(ZHIPU_COOLDOWN)
        else:
            time.sleep(10) # å¤±è´¥æ—¶çš„é»˜è®¤çŸ­ä¼‘æ¯

        # åˆ‡æ¢å¼€å…³ï¼Œä¸‹æ¬¡æ¢å¦ä¸€ä¸ª
        engine_toggle += 1

    if not kept_papers: return "", 0

    # 2. æ’åº (æ¤ç‰©ä¼˜å…ˆ)
    kept_papers.sort(key=lambda p: (
        -p['eval'].get('plant_score', 0), 
        -p['eval'].get('ai_score', 0)
    ))

    # --- Phase 2: æ·±åº¦ç ”è¯» ---
    log(f"\nğŸ§  å¼€å§‹ç²¾è¯» {len(kept_papers)} ç¯‡ (ç»§ç»­æ…¢é€Ÿäº¤æ›¿)...")
    report_content = ""
    
    # ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å¼€å…³çŠ¶æ€ï¼Œä¿æŒäº¤æ›¿
    for i, paper in enumerate(kept_papers):
        current_engine = "gemini" if (engine_toggle % 2 == 0) else "zhipu"
        log(f"   [{i+1}/{len(kept_papers)}] æ·±åº¦ç ”è¯» (é¦–é€‰: {current_engine})...")

        hint = "é‡ç‚¹åˆ†æè¿ç§»åˆ°æ¤ç‰©ç ”ç©¶çš„æ½œåŠ›ã€‚"
        if "METHOD" in paper['eval'].get('tags', []): hint += " å…³æ³¨ç®—æ³•å¯¹ç¨€ç–æ•°æ®çš„é²æ£’æ€§ã€‚"
        
        prompt = DEEP_DIVE_PROMPT_TEMPLATE.format(
            title=paper['title'], source=paper['source'], date=paper['date'],
            tags=",".join(paper['eval'].get('tags', [])), species=paper['eval'].get('species', 'N/A'),
            transfer_hint=hint, abstract=paper['abstract']
        )
        
        summary, used_engine = generate_with_strategy(prompt, preferred_engine=current_engine, is_json=False)
        
        if summary:
            report_content += summary + f"\nğŸ”— **Link**: {paper['url']}\n---\n\n"
        else:
            report_content += f"> âŒ {paper['title']} è§£è¯»å¤±è´¥\n---\n\n"

        # [å…³é”®] å†æ¬¡æ‰§è¡Œä¸¥æ ¼å†·å´
        if used_engine == "gemini":
            log(f"     â³ Gemini å®Œæˆï¼Œä¼‘æ¯ {GEMINI_COOLDOWN} ç§’...")
            time.sleep(GEMINI_COOLDOWN)
        elif used_engine == "zhipu":
            log(f"     â³ æ™ºè°± å®Œæˆï¼Œä¼‘æ¯ {ZHIPU_COOLDOWN} ç§’...")
            time.sleep(ZHIPU_COOLDOWN)
            
        engine_toggle += 1

    return report_content, len(kept_papers)

# æŠ“å–å‡½æ•° (ä¿æŒ)
def fetch_arxiv(seen, limit=10):
    log("ğŸ“¡ [ArXiv] Searching...")
    papers = []
    query = ' OR '.join([f'ti:"{k}"' for k in SEARCH_KEYWORDS[:6]]) 
    try:
        client = arxiv.Client(page_size=limit, delay_seconds=3, num_retries=3)
        search = arxiv.Search(query=query, max_results=limit, sort_by=arxiv.SortCriterion.SubmittedDate)
        for r in client.results(search):
            if not is_duplicate(seen, r.title, "ArXiv"):
                papers.append({"title": r.title, "abstract": r.summary, "url": r.entry_id, "date": r.published.strftime("%Y-%m-%d"), "source": "ArXiv"})
    except Exception as e: log(f"ArXiv Error: {e}")
    return papers

def fetch_biorxiv(seen, limit=10):
    log("ğŸ“¡ [BioRxiv] Searching...")
    papers = []
    try:
        today = datetime.date.today()
        from_date = today - datetime.timedelta(days=5)
        url = f"https://api.biorxiv.org/details/biorxiv/{from_date}/{today}/0/json"
        resp = requests.get(url).json()
        for item in resp.get('collection', [])[:limit*2]: 
            if len(papers) >= limit: break
            if not is_duplicate(seen, item['title'], "BioRxiv"):
                if any(k in (item['title']+item['abstract']).lower() for k in ["single-cell", "plant", "genomics", "deep learning"]):
                    papers.append({"title": item['title'], "abstract": item['abstract'], "url": f"https://doi.org/{item['doi']}", "date": item['date'], "source": "BioRxiv"})
    except Exception as e: log(f"BioRxiv Error: {e}")
    return papers

def fetch_pubmed(seen, limit=5):
    log("ğŸ“¡ [PubMed] Searching...")
    papers = []
    today = datetime.date.today().strftime("%Y/%m/%d")
    past = (datetime.date.today() - datetime.timedelta(days=5)).strftime("%Y/%m/%d")
    term = ' OR '.join([f'({k})' for k in SEARCH_KEYWORDS[:8]]) + f' AND ("{past}"[PDAT] : "{today}"[PDAT])'
    try:
        handle = Entrez.esearch(db="pubmed", term=term, retmax=limit)
        id_list = Entrez.read(handle)["IdList"]
        if not id_list: return []
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="abstract", retmode="xml")
        records = Entrez.read(handle)
        for art in records['PubmedArticle']:
            try:
                data = art['MedlineCitation']['Article']
                title = data['ArticleTitle']
                if not is_duplicate(seen, title, "PubMed"):
                    papers.append({"title": title, "abstract": parse_pubmed_abstract(data), "url": f"https://pubmed.ncbi.nlm.nih.gov/{art['MedlineCitation']['PMID']}/", "date": today, "source": "PubMed"})
            except: pass
    except: pass
    return papers

def main():
    log(f"ğŸš€ Bio-AI Agent v15.0 (Strict Slow-Switch Mode)...")
    seen = set()
    all_p = []
    all_p.extend(fetch_arxiv(seen, 15))
    all_p.extend(fetch_biorxiv(seen, 15))
    all_p.extend(fetch_pubmed(seen, 10))
    
    if not all_p:
        log("No papers found.")
        return

    body, count = process_papers(all_p)
    
    report = f"# ğŸ§  Bio-AI Daily ({datetime.date.today()})\n"
    report += f"> ğŸ“Š Scanned: {len(all_p)} | Selected: {count}\n"
    report += f"> â³ Strategy: Gemini(5m) <-> Zhipu(3m)\n\n"
    if count == 0: report += "No relevant papers today.\n"
    else: report += body

    print(report)
    log("ğŸ“§ æ­£åœ¨å‘é€é‚®ä»¶...")
    send_email(f"Bio-AI Report {datetime.date.today()}", report)

if __name__ == "__main__":
    main()
