import arxiv
import google.generativeai as genai
import datetime
import os
import time
import requests
import re
import sys  # [æ–°å¢] ç”¨äºæ§åˆ¶è¾“å‡ºæµ
from Bio import Entrez

# ==========================================
# 0. æ—¥å¿—è¾…åŠ©å‡½æ•° (æ ¸å¿ƒä¿®å¤)
# ==========================================
def log(msg):
    """
    å°†æ—¥å¿—æ‰“å°åˆ°æ ‡å‡†é”™è¯¯æµ (stderr)ã€‚
    è¿™æ ·åœ¨è¿è¡Œ 'python main.py > report.md' æ—¶ï¼Œ
    æ—¥å¿—ä¼šæ˜¾ç¤ºåœ¨å±å¹•(æ§åˆ¶å°)ä¸Šï¼Œè€Œä¸ä¼šæ±¡æŸ“ report.md æ–‡ä»¶ã€‚
    """
    print(msg, file=sys.stderr)

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸é‰´æƒ
# ==========================================
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
# [å¿…é¡»ä¿®æ”¹] è¯·å¡«å…¥ä½ çš„çœŸå®é‚®ç®±
ENTREZ_EMAIL = "dongwei_li@hotmail.com" 

if not GOOGLE_API_KEY:
    raise ValueError("âŒ æœªæ‰¾åˆ° GOOGLE_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®")

# å¼ºåˆ¶é‚®ç®±æ£€æŸ¥
if "your_real_email" in ENTREZ_EMAIL or "@" not in ENTREZ_EMAIL:
    # ä½¿ç”¨ stderr æ‰“å°é”™è¯¯ï¼Œç¡®ä¿èƒ½çœ‹åˆ°
    log("âŒ é”™è¯¯ï¼šè¯·ä¿®æ”¹ ENTREZ_EMAIL ä¸ºçœŸå®é‚®ç®±ï¼ä½¿ç”¨é»˜è®¤/å‡é‚®ç®±ä¼šå¯¼è‡´ IP è¢« NCBI å°ç¦ã€‚")
    sys.exit(1)

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')
Entrez.email = ENTREZ_EMAIL

# ==========================================
# 2. å¤šæºæ£€ç´¢å…³é”®è¯é…ç½® & æ­£åˆ™é¢„ç¼–è¯‘
# ==========================================
KEYWORDS_FOCUS = {
    "plant_sc": ["plant single cell", "Arabidopsis scRNA-seq", "rice single cell", "crop spatial transcriptomics"],
    "ai_genomics": ["deep learning genomics", "transformer DNA", "genomic foundation model", "DNA language model"],
    "methodology": ["single cell integration", "batch effect correction", "GRN inference", "trajectory inference"]
}

COMPILED_PATTERNS = []
for cat_list in KEYWORDS_FOCUS.values():
    for k in cat_list:
        COMPILED_PATTERNS.append(re.compile(rf'\b{re.escape(k.lower())}\b'))

# ==========================================
# 3. æ ¸å¿ƒæç¤ºè¯
# ==========================================
PAPER_PROMPT_TEMPLATE = """
# Role Assignment
ä½ ç°åœ¨æ˜¯æˆ‘çš„**ç§‘ç ”å‚è°‹ï¼ˆResearch Strategistï¼‰**ã€‚æˆ‘çš„èƒŒæ™¯æ˜¯ï¼š**æ¤ç‰©å•ç»†èƒ + AIè‚²ç§ï¼ˆæ°´ç¨»/æ‹Ÿå—èŠ¥ï¼‰**ã€‚
æˆ‘ä¸éœ€è¦æ–°é—»æŠ¥é“å¼çš„æ€»ç»“ï¼Œæˆ‘éœ€è¦**â€œé»‘å®¢å¼â€çš„æ€è·¯æ‹†è§£**ã€‚

# Task Description
é˜…è¯»è¿™ç¯‡è®ºæ–‡ï¼ˆTitle: {title}ï¼‰ï¼Œæ¥æºï¼š{source}ã€‚
è¯·å…ˆè¿›è¡Œç›¸å…³æ€§åˆç­›ï¼Œè‹¥ç›¸å…³ï¼Œåˆ™è¾“å‡ºä¸€ä»½**é€»è¾‘ä¸¥å¯†ã€æ•°æ®è¯¦å®**çš„æŠ€æœ¯ç ”æŠ¥ã€‚

# Constraints
1. å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¿›è¡Œè¾“å‡ºï¼Œä¿ç•™å¿…è¦çš„è‹±æ–‡ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ Zero-shot, Chain of Thought ç­‰ï¼‰ã€‚
2. ä¸¥ç¦ç›´æ¥ç¿»è¯‘åŸæ–‡æ‘˜è¦ï¼Œå¿…é¡»åŸºäºç†è§£è¿›è¡Œé‡è¿°å’Œæ¦‚æ‹¬ã€‚
3. è¯­æ°”ä¿æŒå®¢è§‚ã€ä¸“ä¸šï¼Œé¿å…ä½¿ç”¨è¥é”€å¼å¤¸å¼ è¯æ±‡ã€‚
4. "åˆ›æ–°ç‚¹"éƒ¨åˆ†å¿…é¡»å…·ä½“ï¼ŒæŒ‡å‡ºè¯¥è®ºæ–‡è§£å†³äº†ä»€ä¹ˆå…·ä½“ç—›ç‚¹ï¼Œä¸ä»…æ˜¯ç½—åˆ—åŠŸèƒ½ã€‚

# Phase 1: Relevance Check (ç›¸å…³æ€§ä¸¥æŸ¥)
è¯·å…ˆåˆ¤æ–­ï¼šè¿™ç¯‡è®ºæ–‡æ˜¯å¦å¯¹â€œæ¤ç‰©ç ”ç©¶â€ã€â€œå•ç»†èƒåˆ†æâ€æˆ–â€œAIåŸºå› ç»„å­¦â€æœ‰å‚è€ƒä»·å€¼ï¼Ÿ
- å¦‚æœå®Œå…¨æ— å…³ï¼ˆå¦‚çº¯ç‰©ç†ã€çº¯ä¸´åºŠè¯ç‰©è¯•éªŒï¼‰ï¼Œè¯·åªè¾“å‡ºä¸€å¥ï¼š"âŒ [ä¸ç›¸å…³] æœ¬æ–‡ä¸»è¦å…³äº...ï¼Œè·³è¿‡ã€‚"
- å¦‚æœç›¸å…³ï¼Œè¯·ç»§ç»­æ‰§è¡Œ Phase 2ã€‚

# Phase 2: Output Format (Strict Markdown)
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š

## ğŸ“‘ [ä¸­æ–‡æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{title}
**æ¥æº**ï¼š{source} | **å‘å¸ƒæ—¶é—´**ï¼š{date}

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[åœ¨æ­¤å¤„æ’°å†™ 150-200 å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚ä¸»è¦æè¿°è®ºæ–‡çš„èƒŒæ™¯é—®é¢˜ã€æå‡ºçš„æ–¹æ³•è®ºä»¥åŠæœ€ç»ˆè¾¾æˆçš„æ•ˆæœã€‚]

### ğŸ§  ç ”ç©¶æ€è·¯å¤ç›˜ (The Logic Chain)
*ä¸è¦åªå‘Šè¯‰æˆ‘ä»–åšäº†ä»€ä¹ˆï¼Œè¦å‘Šè¯‰æˆ‘ä»–æ˜¯æ€ä¹ˆæƒ³åˆ°çš„ã€‚*
* **ğŸ” ç ´å±€ç‚¹ (The Spark)**ï¼šä½œè€…æ˜¯çœ‹åˆ°äº†ä»€ä¹ˆç—›ç‚¹ï¼Œæ‰æƒ³å‡ºäº†è¿™ä¸ªæ–¹æ³•çš„ï¼Ÿ
* **ğŸ› ï¸ æŠ€æœ¯é€‰å‹é€»è¾‘**ï¼šä¸ºä»€ä¹ˆä»–é€‰äº† A æ–¹æ³•è€Œä¸æ˜¯ B æ–¹æ³•ï¼Ÿ
* **â›“ï¸ å®éªŒè®¾è®¡é—­ç¯**ï¼šä»–æ˜¯æ€ä¹ˆè¯æ˜è‡ªå·±æ˜¯å¯¹çš„ï¼Ÿ

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* **[åˆ›æ–°ç‚¹ 1 - æŠ€æœ¯åŸç†]**ï¼šè¯¦ç»†è§£é‡Šè¯¥åˆ›æ–°çš„æŠ€æœ¯åŸç†æˆ–å®ç°æ–¹å¼ï¼Œä»¥åŠå®ƒç›¸å¯¹äºç°æœ‰ SOTA æ–¹æ³•çš„ä¼˜åŠ¿ã€‚
* **[åˆ›æ–°ç‚¹ 2 - å®éªŒè®¾è®¡]**ï¼šæè¿°è¯¥æ–¹æ³•åœ¨å®éªŒè®¾è®¡æˆ–æ•°æ®é›†æ„å»ºä¸Šçš„ç‹¬ç‰¹ä¹‹å¤„ã€‚
* **[åˆ›æ–°ç‚¹ 3 - é‡åŒ–çªç ´]**ï¼šæ€»ç»“è¯¥è®ºæ–‡åœ¨å®éªŒç»“æœä¸Šçš„çªç ´ï¼ˆéœ€åŒ…å«å…·ä½“çš„æå‡æ•°æ®ï¼Œå¦‚ Accuracy æå‡äº† x%ï¼‰ã€‚

### ğŸ™‹â€â™‚ï¸ å¯¹æˆ‘ï¼ˆæ¤ç‰©/å•ç»†èƒï¼‰çš„å€Ÿé‰´ (Actionable Insights)
* **è¿ç§»æ½œåŠ›**ï¼š
    * *å¦‚æœæ˜¯äººç±»/åŠ¨ç‰©ç ”ç©¶*ï¼šè¿™ä¸ªæ€è·¯èƒ½ç›´æ¥å¥—ç”¨åˆ°**æ°´ç¨»/æ‹Ÿå—èŠ¥**ä¸Šå—ï¼Ÿéœ€è¦æ”¹ä»€ä¹ˆï¼Ÿ
    * *å¦‚æœæ˜¯AIç®—æ³•*ï¼šè¿™ä¸ªæ¨¡å‹æ¶æ„é€‚åˆå¤„ç†**æ¤ç‰©åŸºå› ç»„çš„å¤šå€ä½“/é«˜é‡å¤åºåˆ—**ç‰¹å¾å—ï¼Ÿ

### ğŸ“‰ é¿å‘æŒ‡å—
* æ•°æ®è¦æ±‚é«˜å—ï¼Ÿæ˜¾å­˜å ç”¨å¤§å—ï¼Ÿä»£ç å¼€æºäº†å—ï¼Ÿ

---
# Input Data
Title: {title}
Abstract: {abstract}
"""

# ==========================================
# 4. è¾…åŠ©å·¥å…·å‡½æ•°
# ==========================================
def parse_pubmed_abstract(article_data):
    """è§£æ PubMed æ‘˜è¦"""
    abstract_obj = article_data.get('Abstract', {}).get('AbstractText', [])
    if not abstract_obj:
        return "No Abstract"
    
    parts = []
    items = abstract_obj if isinstance(abstract_obj, list) else [abstract_obj]
    
    for item in items:
        if isinstance(item, str):
            parts.append(item)
        elif isinstance(item, dict):
            label = item.get('Label', '')
            text = item.get('#text') or item.get('content') or " ".join([str(v) for v in item.values() if isinstance(v, str)])
            if label:
                parts.append(f"**{label}**: {text}")
            else:
                parts.append(text)
        else:
            parts.append(str(item))
            
    return " ".join(parts)

def is_duplicate(seen_set, title, source):
    """å¤§å°å†™ä¸æ•æ„Ÿå»é‡"""
    key = (title.lower().strip(), source)
    if key in seen_set:
        return True
    seen_set.add(key)
    return False

def contains_keywords(text):
    """ä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¿›è¡Œå…¨è¯åŒ¹é…"""
    text_lower = text.lower()
    for pattern in COMPILED_PATTERNS:
        if pattern.search(text_lower):
            return True
    return False

# ==========================================
# 5. å„å¹³å°æŠ“å–å‡½æ•° (ä½¿ç”¨ log() æ›¿ä»£ print())
# ==========================================

def fetch_arxiv(seen_set, max_results=3):
    log("ğŸ“¡ [ArXiv] æ­£åœ¨è¿æ¥...")
    papers = []
    query = ' OR '.join([f'({k})' for cat in KEYWORDS_FOCUS.values() for k in cat])
    
    client = arxiv.Client(page_size=max_results, delay_seconds=3, num_retries=3)
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    
    try:
        for result in client.results(search):
            if is_duplicate(seen_set, result.title, "ArXiv"): continue
            papers.append({
                "title": result.title,
                "abstract": result.summary,
                "url": result.entry_id,
                "date": result.published.strftime("%Y-%m-%d"),
                "source": "ArXiv"
            })
    except Exception as e:
        log(f"âš ï¸ ArXiv Error: {e}")
    return papers

def fetch_biorxiv(seen_set, limit=4):
    log("ğŸ“¡ [BioRxiv] æ­£åœ¨è¿æ¥...")
    papers = []
    try:
        today = datetime.date.today()
        from_date = today - datetime.timedelta(days=3)
        cursor = "0"
        total_fetched = 0
        
        while True:
            url = f"https://api.biorxiv.org/details/biorxiv/{from_date}/{today}/{cursor}/json"
            resp = requests.get(url).json()
            collection = resp.get('collection', [])
            messages = resp.get('messages', [{}])[0]
            
            if not collection: break
                
            for item in collection:
                if total_fetched >= limit: break
                title = item['title']
                if is_duplicate(seen_set, title, "BioRxiv"): continue
                
                abstract = item['abstract']
                text_to_check = title + " " + abstract
                
                if contains_keywords(text_to_check):
                    papers.append({
                        "title": title,
                        "abstract": abstract,
                        "url": f"https://doi.org/{item['doi']}",
                        "date": item['date'],
                        "source": "BioRxiv"
                    })
                    total_fetched += 1
            
            new_cursor = messages.get('next-cursor')
            
            if not new_cursor or str(new_cursor) == str(cursor) or total_fetched >= limit:
                break
                
            cursor = str(new_cursor)
            time.sleep(1)

    except Exception as e:
        log(f"âš ï¸ BioRxiv Error: {e}")
    return papers

def fetch_pubmed(seen_set, max_results=3):
    log("ğŸ“¡ [PubMed] æ­£åœ¨è¿æ¥...")
    papers = []
    today_str = datetime.date.today().strftime("%Y/%m/%d")
    past_str = (datetime.date.today() - datetime.timedelta(days=3)).strftime("%Y/%m/%d")
    date_term = f' AND ("{past_str}"[PDAT] : "{today_str}"[PDAT])'
    
    term = ' OR '.join([f'({k})' for cat in KEYWORDS_FOCUS.values() for k in cat]) + date_term

    try:
        handle = Entrez.esearch(db="pubmed", term=term, retmax=max_results, sort="date")
        record = Entrez.read(handle)
        id_list = record["IdList"]
        handle.close()

        if not id_list: return []
        time.sleep(3)

        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="abstract", retmode="xml")
        records = Entrez.read(handle)
        handle.close()

        for article in records['PubmedArticle']:
            try:
                article_data = article['MedlineCitation']['Article']
                title = article_data['ArticleTitle']
                if is_duplicate(seen_set, title, "PubMed"): continue

                abstract = parse_pubmed_abstract(article_data)
                pmid = article['MedlineCitation']['PMID']
                
                papers.append({
                    "title": title,
                    "abstract": abstract,
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                    "date": today_str, 
                    "source": "PubMed"
                })
            except Exception as e:
                log(f"âš ï¸ Skip PubMed item: {e}")
                continue
    except Exception as e:
        log(f"âš ï¸ PubMed Error: {e}")
    return papers

# ==========================================
# 6. ä¸»ç¨‹åº
# ==========================================

def process_papers(papers):
    report_content = ""
    for paper in papers:
        # ä½¿ç”¨ log() æ‰“å°è¿›åº¦ï¼Œä¸æ±¡æŸ“æœ€ç»ˆæŠ¥å‘Š
        log(f"ğŸ¤– æ­£åœ¨ç ”è¯» ({paper['source']}): {paper['title'][:40]}...")
        
        prompt = PAPER_PROMPT_TEMPLATE.format(
            title=paper['title'],
            source=paper['source'],
            date=paper['date'],
            abstract=paper['abstract']
        )
        
        try:
            response = model.generate_content(prompt)
            summary = response.text
            
            if "âŒ" in summary and "ä¸ç›¸å…³" in summary:
                log(f"   -> â­ï¸ è·³è¿‡ï¼šå†…å®¹ä¸ç›¸å…³")
                continue
                
            report_content += summary
            report_content += f"\nğŸ”— **åŸæ–‡ç›´è¾¾**: [{paper['source']} Link]({paper['url']})\n"
            report_content += "---\n\n"
            time.sleep(4)
            
        except Exception as e:
            log(f"   -> âŒ åˆ†æå¤±è´¥: {e}")
    return report_content

def main():
    log("ğŸš€ å¯åŠ¨ Bio-AI å…¨ç½‘æƒ…æŠ¥æŠ“å– (v6.0 Final)...")
    seen_papers = set()
    all_papers = []
    
    all_papers.extend(fetch_arxiv(seen_papers, max_results=3))
    all_papers.extend(fetch_biorxiv(seen_papers, limit=4))
    all_papers.extend(fetch_pubmed(seen_papers, max_results=3))
    
    log(f"\nğŸ“Š å…±ç­›é€‰å‡º {len(all_papers)} ç¯‡é«˜ç›¸å…³è®ºæ–‡ï¼Œå¼€å§‹ AI æ·±åº¦ç ”è¯»...\n")
    
    if not all_papers:
        log("ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„æœ€æ–°æ–‡çŒ®æ›´æ–°ã€‚")
        # å³ä½¿æ²¡æœ‰è®ºæ–‡ï¼Œä¹Ÿæ‰“å°ä¸€ä¸ªç©ºçš„æç¤ºï¼Œæˆ–è€…ä»€ä¹ˆéƒ½ä¸æ‰“å°
        return

    daily_report = f"# ğŸ§  Bio-AI æ¯æ—¥æ€è·¯ç ”æŠ¥ ({datetime.date.today()})\n"
    daily_report += "> æ¥æºï¼šArXiv (AI/Method) | BioRxiv (Preprint) | PubMed (Published)\n\n"
    daily_report += process_papers(all_papers)

    # ==========================================
    # å”¯ä¸€çš„ä¸€ä¸ª print (è¾“å‡ºåˆ° stdout)
    # ==========================================
    print(daily_report)

    log("\nâœ… ä»»åŠ¡å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆã€‚")

if __name__ == "__main__":
    main()
