import arxiv
import os
import time
import requests
import re
import sys
import datetime
import json
from Bio import Entrez
from google import genai
from google.genai import types

# ==========================================
# 0. æ—¥å¿—è¾…åŠ©å‡½æ•°
# ==========================================
def log(msg):
    """å°†æ—¥å¿—æ‰“å°åˆ°æ ‡å‡†é”™è¯¯æµ (stderr)"""
    print(msg, file=sys.stderr)

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸é‰´æƒ
# ==========================================
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
# [å¿…é¡»ä¿®æ”¹] å¡«å…¥ä½ çš„çœŸå®é‚®ç®±
ENTREZ_EMAIL = "dongwei_li@hotmail.com"

if not GOOGLE_API_KEY:
    raise ValueError("âŒ æœªæ‰¾åˆ° GOOGLE_API_KEY")

if "@" not in ENTREZ_EMAIL:
    log("âŒ é”™è¯¯ï¼šé‚®ç®±æ ¼å¼ä¸æ­£ç¡®ï¼")
    sys.exit(1)

Entrez.email = ENTREZ_EMAIL

client = genai.Client(api_key=GOOGLE_API_KEY)
# ä½¿ç”¨æ€§ä»·æ¯”æœ€é«˜çš„ç¨³å®šç‰ˆ
MODEL_NAME = "gemini-2.5-flash"

# ==========================================
# 2. æœç´¢ç­–ç•¥ï¼šå®½å¬å› (Broad Recall)
# ==========================================
# æˆ‘ä»¬ä¸å†åœ¨æœç´¢é˜¶æ®µåšæå…¶ä¸¥æ ¼çš„è¿‡æ»¤ï¼Œè€Œæ˜¯å…ˆæŠŠç›¸å…³çš„éƒ½æŠ“å›æ¥ï¼Œè®© LLM å»åˆ¤æ–­
SEARCH_KEYWORDS = [
    # Layer 1: æ ¸å¿ƒæŠ€æœ¯ (åªè¦æ²¾è¾¹å°±æŠ“)
    "single-cell", "scRNA-seq", "spatial transcriptomics", "chromatin accessibility",
    "foundation model", "transformer", "deep learning genomics",
    
    # Layer 2: æ¤ç‰©/ä½œç‰© (ç”¨äºç»„åˆæŸ¥è¯¢)
    "plant", "Arabidopsis", "rice", "maize", "crop breeding"
]

# é¢„ç¼–è¯‘å»é‡æ­£åˆ™
COMPILED_PATTERNS = [re.compile(rf'\b{re.escape(k.lower())}\b') for k in SEARCH_KEYWORDS]

# ==========================================
# 3. Prompt: é˜¶æ®µä¸€ (è£åˆ¤ - è¯„åˆ†ä¸åˆ†ç±»)
# ==========================================
RELEVANCE_PROMPT_TEMPLATE = """
You are a domain expert in **Plant single-cell biology** and **AI-driven crop breeding**.
Your task is to JUDGE the relevance of this paper.

Title: {title}
Abstract: {abstract}

Step 1: Relevance Scoring (0-3)
- Plant relevance (0: None, 3: Core plant study)
- Single-cell/Omics relevance (0: None, 3: Core single-cell/spatial)
- AI/Modeling relevance (0: None, 3: Deep learning/Foundation model)
- Breeding relevance (0: None, 3: Trait prediction/Improvement)

Step 2: Decision
- KEEP: Highly relevant to "Plant Single-Cell" OR "AI Genomics Method".
- MAYBE: Potentially useful method transferable to plants.
- DROP: Pure clinical/cancer study with no transferable method.

Step 3: Tagging (Select all that apply)
- ATLAS (Cell atlas/Reference map)
- METHOD (New computational/experimental method)
- APPLICATION (Biological discovery)
- BREEDING (Trait prediction/Crop improvement)

Output JSON format only:
{{
  "plant_score": int,
  "single_cell_score": int,
  "ai_score": int,
  "breeding_score": int,
  "decision": "KEEP" | "MAYBE" | "DROP",
  "tags": ["TAG1", "TAG2"],
  "reason": "Short reason why"
}}
"""

# ==========================================
# 4. Prompt: é˜¶æ®µäºŒ (å‚è°‹ - æ·±åº¦ç ”è¯»)
# ==========================================
DEEP_DIVE_PROMPT_TEMPLATE = """
# Role
ä½ æ˜¯æˆ‘ï¼ˆæ¤ç‰©å•ç»†èƒ+AIè‚²ç§åšå£«ï¼‰çš„**ç§‘ç ”å‚è°‹**ã€‚
è¿™ç¯‡è®ºæ–‡å·²è¢«åˆ¤å®šä¸º**é«˜ä»·å€¼ ({tags})**ã€‚è¯·è¿›è¡Œé»‘å®¢å¼æ‹†è§£ã€‚

# Metadata
Title: {title}
Tags: {tags}
Relevance Reason: {reason}

# Output Requirements (Strict Markdown)
## ğŸ“‘ [ä¸­æ–‡æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{title}
**æ¥æº**ï¼š{source} | **å‘å¸ƒæ—¶é—´**ï¼š{date}
**æ ‡ç­¾**ï¼š`{tags}`

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[150å­—å·¦å³ï¼ŒèƒŒæ™¯-æ–¹æ³•-ç»“æœ]

### ğŸ§  ç ”ç©¶æ€è·¯å¤ç›˜ (The Logic Chain)
*ä¸è¦åªå‘Šè¯‰æˆ‘ä»–åšäº†ä»€ä¹ˆï¼Œè¦å‘Šè¯‰æˆ‘ä»–æ˜¯æ€ä¹ˆæƒ³åˆ°çš„ã€‚*
* **ğŸ” ç ´å±€ç‚¹ (The Spark)**ï¼šä½œè€…æ˜¯çœ‹åˆ°äº†ä»€ä¹ˆç—›ç‚¹ï¼Œæ‰æƒ³å‡ºäº†è¿™ä¸ªæ–¹æ³•çš„ï¼Ÿ
* **ğŸ› ï¸ æŠ€æœ¯é€‰å‹é€»è¾‘**ï¼šä¸ºä»€ä¹ˆä»–é€‰äº† A æ–¹æ³•è€Œä¸æ˜¯ B æ–¹æ³•ï¼Ÿ
* **â›“ï¸ å®éªŒè®¾è®¡é—­ç¯**ï¼šä»–æ˜¯æ€ä¹ˆè¯æ˜è‡ªå·±æ˜¯å¯¹çš„ï¼Ÿ

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* **[åˆ›æ–°ç‚¹ 1 - æŠ€æœ¯åŸç†]**ï¼šè¯¦ç»†è§£é‡Šè¯¥åˆ›æ–°çš„æŠ€æœ¯åŸç†æˆ–å®ç°æ–¹å¼ï¼Œä»¥åŠå®ƒç›¸å¯¹äºç°æœ‰ SOTA æ–¹æ³•çš„ä¼˜åŠ¿ã€‚
* **[åˆ›æ–°ç‚¹ 2 - å®éªŒè®¾è®¡]**ï¼šæè¿°è¯¥æ–¹æ³•åœ¨å®éªŒè®¾è®¡æˆ–æ•°æ®é›†æ„å»ºä¸Šçš„ç‹¬ç‰¹ä¹‹å¤„ã€‚
* **[åˆ›æ–°ç‚¹ 3 - é‡åŒ–çªç ´]**ï¼šæ€»ç»“è¯¥è®ºæ–‡åœ¨å®éªŒç»“æœä¸Šçš„çªç ´ï¼ˆéœ€åŒ…å«å…·ä½“çš„æå‡æ•°æ®ï¼Œå¦‚ Accuracy æå‡äº† x%ï¼‰ã€‚

### ğŸ™‹â€â™‚ï¸ å¯¹æˆ‘ï¼ˆæ¤ç‰©/å•ç»†èƒï¼‰çš„å€Ÿé‰´ (Actionable Insights)
* **è¿ç§»æ½œåŠ›**ï¼š
    * *å¦‚æœæ˜¯äººç±»/åŠ¨ç‰©ç ”ç©¶*ï¼šè¿™ä¸ªæ€è·¯èƒ½ç›´æ¥å¥—ç”¨åˆ°**æ°´ç¨»/æ‹Ÿå—èŠ¥**ä¸Šå—ï¼Ÿéœ€è¦æ”¹ä»€ä¹ˆï¼Ÿ
    * *å¦‚æœæ˜¯AIç®—æ³•*ï¼šè¿™ä¸ªæ¨¡å‹æ¶æ„é€‚åˆå¤„ç†**æ¤ç‰©åŸºå› ç»„çš„å¤šå€ä½“/é«˜é‡å¤åºåˆ—**ç‰¹å¾å—ï¼Ÿ

### ğŸ“‰ é¿å‘æŒ‡å—
* æ•°æ®è¦æ±‚é«˜å—ï¼Ÿæ˜¾å­˜å ç”¨å¤§å—ï¼Ÿä»£ç å¼€æºäº†å—ï¼Ÿ

---
# Input Abstract
{abstract}
"""

# ==========================================
# 5. å·¥å…·å‡½æ•°
# ==========================================
def parse_pubmed_abstract(article_data):
    """è§£æ PubMed æ‘˜è¦"""
    abstract_obj = article_data.get('Abstract', {}).get('AbstractText', [])
    if not abstract_obj: return "No Abstract"
    parts = []
    items = abstract_obj if isinstance(abstract_obj, list) else [abstract_obj]
    for item in items:
        if isinstance(item, str): parts.append(item)
        elif isinstance(item, dict):
            text = item.get('#text') or item.get('content') or ""
            label = item.get('Label', '')
            parts.append(f"**{label}**: {text}" if label else text)
    return " ".join(parts)

def is_duplicate(seen_set, title, source):
    key = (title.lower().strip(), source)
    if key in seen_set: return True
    seen_set.add(key)
    return False

# ==========================================
# 6. æ ¸å¿ƒé€»è¾‘ï¼šAI è£åˆ¤ (Judge)
# ==========================================
def evaluate_paper_relevance(paper):
    """è°ƒç”¨ Gemini åˆ¤æ–­è®ºæ–‡æ˜¯å¦å€¼å¾—è¯»ï¼Œè¿”å› JSON"""
    prompt = RELEVANCE_PROMPT_TEMPLATE.format(
        title=paper['title'],
        abstract=paper['abstract']
    )
    try:
        # å¼ºåˆ¶è¾“å‡º JSON
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json"
            )
        )
        return json.loads(response.text)
    except Exception as e:
        log(f"âš ï¸ è¯„åˆ†å¤±è´¥: {e}")
        # é»˜è®¤æ”¾è¡Œï¼Œé˜²æ­¢æ¼æ‰ï¼Œæ ‡è®°ä¸º MAYBE
        return {"decision": "MAYBE", "tags": ["ERROR"], "reason": "JSON parse error"}

# ==========================================
# 7. æ ¸å¿ƒé€»è¾‘ï¼šAI å‚è°‹ (Analyst)
# ==========================================
def generate_deep_dive(paper, evaluation):
    """å¯¹é«˜åˆ†è®ºæ–‡è¿›è¡Œæ·±åº¦è§£è¯»"""
    # åŠ¨æ€è°ƒæ•´ Promptï¼šå¦‚æœæ˜¯çº¯ AI æ–¹æ³•ï¼Œå¼ºè°ƒè¿ç§»æ€§
    transfer_hint = "å¦‚æœæ˜¯äººç±»ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æå¦‚ä½•è¿ç§»åˆ°æ¤ç‰©ç»†èƒå£/å¤šå€ä½“åœºæ™¯ã€‚"
    if "METHOD" in evaluation['tags']:
        transfer_hint += " é‡ç‚¹å…³æ³¨ç®—æ³•æ˜¯å¦èƒ½å¤„ç†æ¤ç‰©æ•°æ®çš„ç¨€ç–æ€§ã€‚"

    prompt = DEEP_DIVE_PROMPT_TEMPLATE.format(
        title=paper['title'],
        source=paper['source'],
        date=paper['date'],
        tags=",".join(evaluation['tags']),
        reason=evaluation['reason'],
        transfer_hint=transfer_hint,
        abstract=paper['abstract']
    )
    
    try:
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt
        )
        return response.text
    except Exception as e:
        log(f"âŒ æ·±åº¦è§£è¯»å¤±è´¥: {e}")
        return f"> âŒ è§£è¯»å‡ºé”™: {e}"

# ==========================================
# 8. æŠ“å–å‡½æ•° (å®½æœç´¢)
# ==========================================
def fetch_arxiv(seen_set, max_results=10): # æŠ“å¤šç‚¹ï¼Œè®© AI ç­›
    log("ğŸ“¡ [ArXiv] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    # æ„é€ æ›´å®½çš„æŸ¥è¯¢ï¼š(Single Cell OR AI) AND (Plant OR Deep Learning)
    # è¿™é‡Œæˆ‘ä»¬ç¨å¾®æ”¾å®½ï¼Œåªè¦åŒ…å«æ ¸å¿ƒè¯å³å¯
    query = ' OR '.join([f'ti:"{k}"' for k in SEARCH_KEYWORDS[:5]]) + \
            ' OR ' + ' OR '.join([f'abs:"{k}"' for k in SEARCH_KEYWORDS[:5]])
    
    client_arxiv = arxiv.Client(page_size=max_results, delay_seconds=3, num_retries=3)
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    
    try:
        for result in client_arxiv.results(search):
            if is_duplicate(seen_set, result.title, "ArXiv"): continue
            papers.append({
                "title": result.title,
                "abstract": result.summary,
                "url": result.entry_id,
                "date": result.published.strftime("%Y-%m-%d"),
                "source": "ArXiv"
            })
    except Exception as e:
        log(f"âš ï¸ ArXiv Error: {e}")
    return papers

def fetch_biorxiv(seen_set, limit=10): # æŠ“å¤šç‚¹
    log("ğŸ“¡ [BioRxiv] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    try:
        today = datetime.date.today()
        from_date = today - datetime.timedelta(days=7) # 7å¤©
        cursor = "0"
        total_fetched = 0
        
        while True:
            url = f"https://api.biorxiv.org/details/biorxiv/{from_date}/{today}/{cursor}/json"
            resp = requests.get(url).json()
            collection = resp.get('collection', [])
            messages = resp.get('messages', [{}])[0]
            
            if not collection: break
                
            for item in collection:
                if total_fetched >= limit: break
                title = item['title']
                if is_duplicate(seen_set, title, "BioRxiv"): continue
                
                # æœ¬åœ°ç®€å•å…³é”®è¯åˆç­› (Layer 0)ï¼Œé¿å…é€ç»™ LLM å¤ªå¤šåƒåœ¾
                text_check = (title + item['abstract']).lower()
                if any(k.lower() in text_check for k in SEARCH_KEYWORDS):
                    papers.append({
                        "title": title,
                        "abstract": item['abstract'],
                        "url": f"https://doi.org/{item['doi']}",
                        "date": item['date'],
                        "source": "BioRxiv"
                    })
                    total_fetched += 1
            
            new_cursor = messages.get('next-cursor')
            if not new_cursor or str(new_cursor) == str(cursor) or total_fetched >= limit: break
            cursor = str(new_cursor)
            time.sleep(1)
    except Exception as e:
        log(f"âš ï¸ BioRxiv Error: {e}")
    return papers

def fetch_pubmed(seen_set, max_results=5):
    log("ğŸ“¡ [PubMed] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    today_str = datetime.date.today().strftime("%Y/%m/%d")
    past_str = (datetime.date.today() - datetime.timedelta(days=7)).strftime("%Y/%m/%d")
    date_term = f' AND ("{past_str}"[PDAT] : "{today_str}"[PDAT])'
    
    # æ„é€ æŸ¥è¯¢ï¼š(plant OR single cell OR AI)
    term = ' OR '.join([f'({k})' for k in SEARCH_KEYWORDS]) + date_term

    try:
        handle = Entrez.esearch(db="pubmed", term=term, retmax=max_results, sort="date")
        record = Entrez.read(handle)
        id_list = record["IdList"]
        handle.close()
        if not id_list: return []
        time.sleep(2)
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="abstract", retmode="xml")
        records = Entrez.read(handle)
        handle.close()

        for article in records['PubmedArticle']:
            try:
                article_data = article['MedlineCitation']['Article']
                title = article_data['ArticleTitle']
                if is_duplicate(seen_set, title, "PubMed"): continue
                papers.append({
                    "title": title,
                    "abstract": parse_pubmed_abstract(article_data),
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{article['MedlineCitation']['PMID']}/",
                    "date": today_str,
                    "source": "PubMed"
                })
            except: continue
    except Exception as e:
        log(f"âš ï¸ PubMed Error: {e}")
    return papers

# ==========================================
# 9. ä¸»æµç¨‹ (Pipeline)
# ==========================================
def process_papers(papers):
    report_content = ""
    kept_count = 0
    
    for paper in papers:
        log(f"ğŸ¤– [è£åˆ¤] æ­£åœ¨è¯„å®¡: {paper['title'][:30]}...")
        
        # Step 1: è£åˆ¤æ‰“åˆ†
        eval_result = evaluate_paper_relevance(paper)
        decision = eval_result.get('decision', 'DROP')
        tags = eval_result.get('tags', [])
        
        # è°ƒè¯•æ—¥å¿—
        log(f"   -> ç»“æœ: {decision} | æ ‡ç­¾: {tags}")
        
        # Step 2: è¿‡æ»¤
        if decision == "DROP":
            continue
            
        kept_count += 1
        log(f"ğŸ§  [å‚è°‹] æ­£åœ¨æ·±åº¦ç ”è¯»...")
        
        # Step 3: æ·±åº¦ç ”è¯»
        summary = generate_deep_dive(paper, eval_result)
        
        # ç»“æœæ‹¼æ¥
        report_content += summary
        report_content += f"\nğŸ”— **åŸæ–‡ç›´è¾¾**: [{paper['source']} Link]({paper['url']})\n"
        report_content += f"> ğŸ·ï¸ **è‡ªåŠ¨æ ‡ç­¾**: `{', '.join(tags)}` | ğŸ“Š **AIè¯„åˆ†**: Plant({eval_result.get('plant_score')}) AI({eval_result.get('ai_score')})\n"
        report_content += "---\n\n"
        
        time.sleep(2)

    return report_content, kept_count

def main():
    log(f"ğŸš€ å¯åŠ¨ Bio-AI æƒ…æŠ¥ Agent (v11.0 Architect)...")
    seen_papers = set()
    all_papers = []
    
    # 1. å®½èŒƒå›´æŠ“å– (æ•°é‡è®¾å¤§ä¸€ç‚¹ï¼Œè®© AI ç­›)
    all_papers.extend(fetch_arxiv(seen_papers, max_results=10))
    all_papers.extend(fetch_biorxiv(seen_papers, limit=10))
    all_papers.extend(fetch_pubmed(seen_papers, max_results=5))
    
    log(f"\nğŸ“Š å®½å¬å›é˜¶æ®µï¼šå…±è·å– {len(all_papers)} ç¯‡å€™é€‰è®ºæ–‡ï¼Œå¼€å§‹ AI è¯„å®¡...\n")
    
    if not all_papers:
        log("æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ã€‚")
        return

    # 2. æ™ºèƒ½è¯„å®¡ä¸ç ”è¯»
    report_body, kept_count = process_papers(all_papers)

    # 3. ç”ŸæˆæŠ¥å‘Šå¤´
    daily_report = f"# ğŸ§  Bio-AI æ¯æ—¥æƒ…æŠ¥å†³ç­– ({datetime.date.today()})\n"
    daily_report += f"> ğŸ“Š ä»Šæ—¥å¤§ç›˜ï¼šå¬å› {len(all_papers)} ç¯‡ -> AI ä¸¥é€‰ {kept_count} ç¯‡\n"
    daily_report += "> ğŸ¤– æ¶æ„ï¼šBroad Recall -> Relevance Scoring -> Deep Dive\n\n"
    
    if kept_count == 0:
        daily_report += "### ä»Šæ—¥æ— é«˜ä»·å€¼è®ºæ–‡å…¥é€‰\n"
        daily_report += "è™½ç„¶æŠ“å–äº†å€™é€‰è®ºæ–‡ï¼Œä½†ç» AI è£åˆ¤è¯„å®¡ï¼Œå‡æœªè¾¾åˆ° KEEP æ ‡å‡†ï¼ˆç›¸å…³æ€§ä¸è¶³ï¼‰ã€‚å»ºè®®æ˜å¤©ç»§ç»­å…³æ³¨ã€‚\n"
    else:
        daily_report += report_body

    print(daily_report)
    log("\nâœ… ä»»åŠ¡å®Œæˆã€‚")

if __name__ == "__main__":
    main()
