import arxiv
import os
import time
import requests
import re
import sys
import datetime
import json
from Bio import Entrez
from google import genai
from google.genai import types

# ==========================================
# 0. æ—¥å¿—ä¸é…ç½®
# ==========================================
def log(msg):
    """å°†æ—¥å¿—æ‰“å°åˆ°æ ‡å‡†é”™è¯¯æµ (stderr)"""
    print(msg, file=sys.stderr)

# è·å– Key
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
ZHIPU_API_KEY = os.getenv('ZHIPU_API_KEY') # [æ–°å¢] ä»ç¯å¢ƒå˜é‡è·å–
ENTREZ_EMAIL = "dongwei_li@hotmail.com"

# æ£€æŸ¥é…ç½®
if not GOOGLE_API_KEY:
    raise ValueError("âŒ æœªæ‰¾åˆ° GOOGLE_API_KEY")
if not ZHIPU_API_KEY:
    log("âš ï¸ æœªæ‰¾åˆ° ZHIPU_API_KEYï¼Œå°†ä»…ä½¿ç”¨ Gemini æ¨¡å¼")

if "@" not in ENTREZ_EMAIL:
    log("âŒ é”™è¯¯ï¼šé‚®ç®±æ ¼å¼ä¸æ­£ç¡®ï¼")
    sys.exit(1)

Entrez.email = ENTREZ_EMAIL

# åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
client_gemini = genai.Client(api_key=GOOGLE_API_KEY)
GEMINI_MODEL = "gemini-2.5-flash"

# ==========================================
# 1. åŒæ¨¡å‹åº•å±‚å°è£… (æ ¸å¿ƒå‡çº§)
# ==========================================

def call_gemini(prompt, is_json=False):
    """è°ƒç”¨ Google Gemini"""
    try:
        config = types.GenerateContentConfig(response_mime_type="application/json") if is_json else None
        response = client_gemini.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=config
        )
        return response.text
    except Exception as e:
        # æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚æ•è·ï¼Œä»¥ä¾¿åˆ‡æ¢æ¨¡å‹
        raise e

def call_zhipu(prompt, is_json=False):
    """è°ƒç”¨æ™ºè°± GLM-4 (ä½¿ç”¨ä½ æä¾›çš„ requests æ–¹å¼)"""
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    
    # æ„é€  GLM çš„ prompt æ ¼å¼
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦ç§‘ç ”åŠ©æ‰‹ã€‚è¯·ç›´æ¥è¾“å‡ºç»“æœï¼Œä¸è¦åºŸè¯ã€‚"},
        {"role": "user", "content": prompt}
    ]
    
    # å¦‚æœéœ€è¦ JSONï¼Œæˆ‘ä»¬åœ¨ System Prompt é‡Œå¼ºè°ƒä¸€ä¸‹ï¼ˆGLM-4-Flash å¯¹ JSON mode æ”¯æŒè§†ç‰ˆæœ¬è€Œå®šï¼Œè¿™é‡Œé€šè¿‡ Prompt çº¦æŸï¼‰
    if is_json:
        messages[0]["content"] += " è¯·åŠ¡å¿…è¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®°ã€‚"

    payload = {
        "model": "glm-4-flash", # ä½¿ç”¨æ€§ä»·æ¯”é«˜çš„ Flash ç‰ˆæœ¬
        "messages": messages,
        "stream": False,
        "temperature": 0.5, # ç§‘ç ”ä»»åŠ¡ç¨å¾®é™ä½åˆ›é€ æ€§
        "thinking": { "type": "disabled" } # æš‚æ—¶å…³æ‰ thinking ä»¥å…å¹²æ‰° JSON è§£æï¼Œé™¤éä½ éœ€è¦æ€ç»´é“¾
    }
    
    headers = {
        "Authorization": f"Bearer {ZHIPU_API_KEY}",
        "Content-Type": "application/json"
    }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=60) # è®¾ç½®60ç§’è¶…æ—¶
        resp_json = resp.json()
        
        # è§£æå“åº”
        if "choices" in resp_json:
            content = resp_json['choices'][0]['message']['content']
            #ä»¥æ­¤æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
            content = content.replace("```json", "").replace("```", "").strip()
            return content
        elif "error" in resp_json:
            raise Exception(f"Zhipu API Error: {resp_json['error']}")
        else:
            raise Exception(f"Unknown Zhipu Response: {resp.text}")
            
    except Exception as e:
        raise e

def hybrid_generate_content(prompt, is_json=False):
    """
    [æ™ºèƒ½æ··åˆè°ƒç”¨]
    ç­–ç•¥ï¼šä¼˜å…ˆ Gemini -> å¤±è´¥/é™æµ -> åˆ‡æ¢ GLM-4 -> å†å¤±è´¥ -> ä¼‘æ¯é‡è¯•
    """
    # 1. å°è¯• Gemini (ä¸»åŠ›)
    try:
        # log("   âš¡ [Gemini] æ­£åœ¨æ€è€ƒ...")
        return call_gemini(prompt, is_json)
    except Exception as e:
        error_str = str(e)
        
        # 2. å¦‚æœ Gemini æŒ‚äº† (429 é™æµ æˆ– 500 é”™è¯¯)ï¼Œä¸”é…ç½®äº†æ™ºè°± Key
        if ("429" in error_str or "RESOURCE_EXHAUSTED" in error_str) and ZHIPU_API_KEY:
            log(f"   âš ï¸ Gemini é™æµ/æŠ¥é”™ï¼Œè‡ªåŠ¨åˆ‡æ¢è‡³ [æ™ºè°±GLM-4] æ¥åŠ›...")
            try:
                return call_zhipu(prompt, is_json)
            except Exception as e_zhipu:
                log(f"   âŒ æ™ºè°±ä¹ŸæŒ‚äº†: {e_zhipu}")
                # ä¸¤ä¸ªéƒ½æŒ‚äº†ï¼Œåªèƒ½ä¼‘æ¯ç­‰å¾…äº†
                time.sleep(30)
                return None
        
        # å¦‚æœæ²¡é…ç½®æ™ºè°± Keyï¼Œåªèƒ½ç¡¬ç­‰
        elif "429" in error_str:
             log(f"   âš ï¸ Gemini é™æµï¼Œæ— å¤‡ç”¨æ¨¡å‹ï¼Œç­‰å¾… 30ç§’...")
             time.sleep(30)
             return None
        else:
            log(f"   âŒ API æœªçŸ¥é”™è¯¯: {e}")
            return None

# ==========================================
# 2. æœç´¢ç­–ç•¥
# ==========================================
SEARCH_KEYWORDS = [
    "single-cell", "scRNA-seq", "spatial transcriptomics", "chromatin accessibility",
    "foundation model", "transformer", "deep learning genomics",
    "plant", "Arabidopsis", "rice", "maize", "crop breeding"
]
COMPILED_PATTERNS = [re.compile(rf'\b{re.escape(k.lower())}\b') for k in SEARCH_KEYWORDS]

# ==========================================
# 3. Prompt: é˜¶æ®µä¸€ (è£åˆ¤ - è¯„åˆ†ä¸åˆ†ç±»)
# ==========================================
RELEVANCE_PROMPT_TEMPLATE = """
You are a domain expert in **Plant single-cell biology** and **AI-driven crop breeding**.
Your task is to JUDGE the relevance of this paper.

Title: {title}
Abstract: {abstract}

Step 1: Relevance Scoring (0-3)
- Plant relevance (0: None, 3: Core plant study)
- Single-cell/Omics relevance (0: None, 3: Core single-cell/spatial)
- AI/Modeling relevance (0: None, 3: Deep learning/Foundation model)
- Breeding relevance (0: None, 3: Trait prediction/Improvement)

Step 2: Extract Species
- Extract the main organism/species studied (e.g., "Rice (Oryza sativa)", "Arabidopsis", "Human", "General Model").

Step 3: Decision
- KEEP: Highly relevant.
- DROP: Totally irrelevant.

Step 4: Tagging
- ATLAS, METHOD, APPLICATION, BREEDING

Output JSON format only:
{{
  "plant_score": int,
  "single_cell_score": int,
  "ai_score": int,
  "breeding_score": int,
  "species": "String",
  "decision": "KEEP" | "DROP",
  "tags": ["TAG1", "TAG2"],
  "reason": "Short reason"
}}
"""

# ==========================================
# 4. Prompt: é˜¶æ®µäºŒ (å‚è°‹ - æ·±åº¦ç ”è¯»)
# ==========================================
DEEP_DIVE_PROMPT_TEMPLATE = """
# Role
ä½ æ˜¯æˆ‘ï¼ˆæ¤ç‰©å•ç»†èƒ+AIè‚²ç§åšå£«ï¼‰çš„**ç§‘ç ”å‚è°‹**ã€‚
è¿™ç¯‡è®ºæ–‡å·²è¢«åˆ¤å®šä¸º**é«˜ä»·å€¼**ã€‚è¯·è¿›è¡Œé»‘å®¢å¼æ‹†è§£ã€‚

# Metadata
Title: {title}
Species: {species}
Tags: {tags}

# Output Requirements (Strict Markdown)
## ğŸ“‘ [ä¸­æ–‡æ ‡é¢˜]
**åŸæ ‡é¢˜**ï¼š{title}
**æ¥æº**ï¼š{source} | **å‘å¸ƒæ—¶é—´**ï¼š{date}
**ç ”ç©¶ç‰©ç§**ï¼š`{species}` | **æ ‡ç­¾**ï¼š`{tags}`

### ğŸ¯ æ ¸å¿ƒæ‘˜è¦
[åœ¨æ­¤å¤„æ’°å†™ 150-200 å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚ä¸»è¦æè¿°è®ºæ–‡çš„èƒŒæ™¯é—®é¢˜ã€æå‡ºçš„æ–¹æ³•è®ºä»¥åŠæœ€ç»ˆè¾¾æˆçš„æ•ˆæœã€‚]

### ğŸ§  ç ”ç©¶æ€è·¯å¤ç›˜ (The Logic Chain)
*ä¸è¦åªå‘Šè¯‰æˆ‘ä»–åšäº†ä»€ä¹ˆï¼Œè¦å‘Šè¯‰æˆ‘ä»–æ˜¯æ€ä¹ˆæƒ³åˆ°çš„ã€‚*
* **ğŸ” ç ´å±€ç‚¹ (The Spark)**ï¼šä½œè€…æ˜¯çœ‹åˆ°äº†ä»€ä¹ˆç—›ç‚¹ï¼Œæ‰æƒ³å‡ºäº†è¿™ä¸ªæ–¹æ³•çš„ï¼Ÿ
* **ğŸ› ï¸ æŠ€æœ¯é€‰å‹é€»è¾‘**ï¼šä¸ºä»€ä¹ˆä»–é€‰äº† A æ–¹æ³•è€Œä¸æ˜¯ B æ–¹æ³•ï¼Ÿ
* **â›“ï¸ å®éªŒè®¾è®¡é—­ç¯**ï¼šä»–æ˜¯æ€ä¹ˆè¯æ˜è‡ªå·±æ˜¯å¯¹çš„ï¼Ÿ

### ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹ä¸è´¡çŒ®
* **[åˆ›æ–°ç‚¹ 1 - æŠ€æœ¯åŸç†]**ï¼šè¯¦ç»†è§£é‡Šè¯¥åˆ›æ–°çš„æŠ€æœ¯åŸç†æˆ–å®ç°æ–¹å¼ï¼Œä»¥åŠå®ƒç›¸å¯¹äºç°æœ‰ SOTA æ–¹æ³•çš„ä¼˜åŠ¿ã€‚
* **[åˆ›æ–°ç‚¹ 2 - å®éªŒè®¾è®¡]**ï¼šæè¿°è¯¥æ–¹æ³•åœ¨å®éªŒè®¾è®¡æˆ–æ•°æ®é›†æ„å»ºä¸Šçš„ç‹¬ç‰¹ä¹‹å¤„ã€‚
* **[åˆ›æ–°ç‚¹ 3 - é‡åŒ–çªç ´]**ï¼šæ€»ç»“è¯¥è®ºæ–‡åœ¨å®éªŒç»“æœä¸Šçš„çªç ´ï¼ˆéœ€åŒ…å«å…·ä½“çš„æå‡æ•°æ®ï¼Œå¦‚ Accuracy æå‡äº† x%ï¼‰ã€‚

### ğŸ™‹â€â™‚ï¸ å¯¹æˆ‘ï¼ˆæ¤ç‰©/å•ç»†èƒï¼‰çš„å€Ÿé‰´ (Actionable Insights)
* **è¿ç§»æ½œåŠ›**ï¼š
    * *å¦‚æœæ˜¯äººç±»/åŠ¨ç‰©ç ”ç©¶*ï¼šè¿™ä¸ªæ€è·¯èƒ½ç›´æ¥å¥—ç”¨åˆ°**æ°´ç¨»/æ‹Ÿå—èŠ¥**ä¸Šå—ï¼Ÿéœ€è¦æ”¹ä»€ä¹ˆï¼Ÿ
    * *å¦‚æœæ˜¯AIç®—æ³•*ï¼šè¿™ä¸ªæ¨¡å‹æ¶æ„é€‚åˆå¤„ç†**æ¤ç‰©åŸºå› ç»„çš„å¤šå€ä½“/é«˜é‡å¤åºåˆ—**ç‰¹å¾å—ï¼Ÿ

### ğŸ“‰ é¿å‘æŒ‡å—
* æ•°æ®è¦æ±‚é«˜å—ï¼Ÿæ˜¾å­˜å ç”¨å¤§å—ï¼Ÿä»£ç å¼€æºäº†å—ï¼Ÿ

---
# Input Abstract
{abstract}
"""

# ==========================================
# 5. å·¥å…·å‡½æ•°
# ==========================================
def parse_pubmed_abstract(article_data):
    abstract_obj = article_data.get('Abstract', {}).get('AbstractText', [])
    if not abstract_obj: return "No Abstract"
    parts = []
    items = abstract_obj if isinstance(abstract_obj, list) else [abstract_obj]
    for item in items:
        if isinstance(item, str): parts.append(item)
        elif isinstance(item, dict):
            text = item.get('#text') or item.get('content') or ""
            label = item.get('Label', '')
            parts.append(f"**{label}**: {text}" if label else text)
    return " ".join(parts)

def is_duplicate(seen_set, title, source):
    key = (title.lower().strip(), source)
    if key in seen_set: return True
    seen_set.add(key)
    return False

# ==========================================
# 6. æ ¸å¿ƒé€»è¾‘ï¼šAI è£åˆ¤ (Judge)
# ==========================================
def evaluate_paper_relevance(paper):
    """è°ƒç”¨æ··åˆæ¨¡å‹åˆ¤æ–­è®ºæ–‡"""
    prompt = RELEVANCE_PROMPT_TEMPLATE.format(
        title=paper['title'],
        abstract=paper['abstract']
    )
    
    # ä½¿ç”¨æ··åˆè°ƒç”¨æ¥å£
    response_text = hybrid_generate_content(prompt, is_json=True)
    
    if response_text:
        try:
            return json.loads(response_text)
        except:
            # ç®€å•çš„ JSON ä¿®å¤å°è¯•
            try:
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                return json.loads(response_text[start:end])
            except:
                return {"decision": "KEEP", "tags": ["PARSE_ERROR"], "species": "Unknown", "reason": "JSON Error"}
    return {"decision": "DROP", "tags": [], "reason": "API Error"}

# ==========================================
# 7. æ ¸å¿ƒé€»è¾‘ï¼šAI å‚è°‹ (Analyst)
# ==========================================
def generate_deep_dive(paper, evaluation):
    """æ·±åº¦è§£è¯»"""
    transfer_hint = "å¦‚æœæ˜¯äººç±»ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æå¦‚ä½•è¿ç§»åˆ°æ¤ç‰©ç»†èƒå£/å¤šå€ä½“åœºæ™¯ã€‚"
    if "METHOD" in evaluation['tags']:
        transfer_hint += " é‡ç‚¹å…³æ³¨ç®—æ³•æ˜¯å¦èƒ½å¤„ç†æ¤ç‰©æ•°æ®çš„ç¨€ç–æ€§ã€‚"

    prompt = DEEP_DIVE_PROMPT_TEMPLATE.format(
        title=paper['title'],
        source=paper['source'],
        date=paper['date'],
        tags=",".join(evaluation['tags']),
        species=evaluation.get('species', 'N/A'),
        transfer_hint=transfer_hint,
        abstract=paper['abstract']
    )
    
    # ä½¿ç”¨æ··åˆè°ƒç”¨æ¥å£
    response_text = hybrid_generate_content(prompt, is_json=False)
    
    if response_text:
        return response_text
    return f"> âŒ è§£è¯»å¤±è´¥ï¼šæ‰€æœ‰æ¨¡å‹å‡æ— å“åº”ã€‚"

# ==========================================
# 8. æŠ“å–å‡½æ•°
# ==========================================
def fetch_arxiv(seen_set, max_results=10):
    log("ğŸ“¡ [ArXiv] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    query = ' OR '.join([f'ti:"{k}"' for k in SEARCH_KEYWORDS[:5]]) + \
            ' OR ' + ' OR '.join([f'abs:"{k}"' for k in SEARCH_KEYWORDS[:5]])
    client_arxiv = arxiv.Client(page_size=max_results, delay_seconds=3, num_retries=3)
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)
    try:
        for result in client_arxiv.results(search):
            if is_duplicate(seen_set, result.title, "ArXiv"): continue
            papers.append({
                "title": result.title, "abstract": result.summary,
                "url": result.entry_id, "date": result.published.strftime("%Y-%m-%d"),
                "source": "ArXiv"
            })
    except Exception as e: log(f"âš ï¸ ArXiv Error: {e}")
    return papers

def fetch_biorxiv(seen_set, limit=10):
    log("ğŸ“¡ [BioRxiv] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    try:
        today = datetime.date.today()
        from_date = today - datetime.timedelta(days=7)
        cursor = "0"
        total_fetched = 0
        while True:
            url = f"https://api.biorxiv.org/details/biorxiv/{from_date}/{today}/{cursor}/json"
            resp = requests.get(url).json()
            collection = resp.get('collection', [])
            messages = resp.get('messages', [{}])[0]
            if not collection: break
            for item in collection:
                if total_fetched >= limit: break
                title = item['title']
                if is_duplicate(seen_set, title, "BioRxiv"): continue
                text_check = (title + item['abstract']).lower()
                if any(k.lower() in text_check for k in SEARCH_KEYWORDS):
                    papers.append({
                        "title": title, "abstract": item['abstract'],
                        "url": f"https://doi.org/{item['doi']}", "date": item['date'],
                        "source": "BioRxiv"
                    })
                    total_fetched += 1
            new_cursor = messages.get('next-cursor')
            if not new_cursor or str(new_cursor) == str(cursor) or total_fetched >= limit: break
            cursor = str(new_cursor)
            time.sleep(1)
    except Exception as e: log(f"âš ï¸ BioRxiv Error: {e}")
    return papers

def fetch_pubmed(seen_set, max_results=5):
    log("ğŸ“¡ [PubMed] å®½èŒƒå›´æœç´¢ä¸­...")
    papers = []
    today_str = datetime.date.today().strftime("%Y/%m/%d")
    past_str = (datetime.date.today() - datetime.timedelta(days=7)).strftime("%Y/%m/%d")
    date_term = f' AND ("{past_str}"[PDAT] : "{today_str}"[PDAT])'
    term = ' OR '.join([f'({k})' for k in SEARCH_KEYWORDS]) + date_term
    try:
        handle = Entrez.esearch(db="pubmed", term=term, retmax=max_results, sort="date")
        record = Entrez.read(handle)
        id_list = record["IdList"]
        handle.close()
        if not id_list: return []
        time.sleep(2)
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="abstract", retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        for article in records['PubmedArticle']:
            try:
                article_data = article['MedlineCitation']['Article']
                title = article_data['ArticleTitle']
                if is_duplicate(seen_set, title, "PubMed"): continue
                papers.append({
                    "title": title, "abstract": parse_pubmed_abstract(article_data),
                    "url": f"https://pubmed.ncbi.nlm.nih.gov/{article['MedlineCitation']['PMID']}/",
                    "date": today_str, "source": "PubMed"
                })
            except: continue
    except Exception as e: log(f"âš ï¸ PubMed Error: {e}")
    return papers

# ==========================================
# 9. ä¸»æµç¨‹
# ==========================================
def process_papers(papers):
    
    # 1. è¯„åˆ†é˜¶æ®µ
    log(f"âš–ï¸ å¼€å§‹ç¬¬ä¸€è½®ç­›é€‰ (å…± {len(papers)} ç¯‡)...")
    kept_papers = []
    
    for paper in papers:
        eval_result = evaluate_paper_relevance(paper)
        
        decision = eval_result.get('decision', 'DROP')
        species = eval_result.get('species', 'N/A')
        log(f"   -> {paper['title'][:20]}... | {decision} | {species}")
        
        if decision == "KEEP":
            paper['eval'] = eval_result
            kept_papers.append(paper)
        
        time.sleep(1) # ç¨å¾®å‡å°‘ç­‰å¾…ï¼Œå› ä¸ºæœ‰åŒæ¨¡å‹åˆ‡æ¢ä¿éšœ

    if not kept_papers:
        return "", 0

    # 2. æ’åºé˜¶æ®µ
    log("ğŸ”„ æ­£åœ¨æ™ºèƒ½æ’åº...")
    def sort_key(p):
        plant_score = p['eval'].get('plant_score', 0)
        ai_score = p['eval'].get('ai_score', 0)
        if plant_score >= 2: return (0, -plant_score, -ai_score)
        elif ai_score >= 2: return (1, -ai_score, -plant_score)
        else: return (2, -ai_score, -plant_score)
            
    kept_papers.sort(key=sort_key)

    # 3. ç ”è¯»é˜¶æ®µ
    log(f"ğŸ§  å¼€å§‹æ·±åº¦ç ”è¯» (å…¥é€‰ {len(kept_papers)} ç¯‡)...")
    report_content = ""
    
    for paper in kept_papers:
        summary = generate_deep_dive(paper, paper['eval'])
        
        report_content += summary
        report_content += f"\nğŸ”— **åŸæ–‡ç›´è¾¾**: [{paper['source']} Link]({paper['url']})\n"
        tags = paper['eval'].get('tags', [])
        plant_score = paper['eval'].get('plant_score', 0)
        ai_score = paper['eval'].get('ai_score', 0)
        report_content += f"> ğŸ·ï¸ **è‡ªåŠ¨æ ‡ç­¾**: `{', '.join(tags)}` | ğŸ“Š **è¯„åˆ†**: Plant({plant_score}) AI({ai_score})\n"
        report_content += "---\n\n"
        
        time.sleep(2) 

    return report_content, len(kept_papers)

def main():
    log(f"ğŸš€ å¯åŠ¨ Bio-AI æƒ…æŠ¥ Agent (v13.0 Hybrid: Gemini + Zhipu)...")
    seen_papers = set()
    all_papers = []
    
    all_papers.extend(fetch_arxiv(seen_papers, max_results=10))
    all_papers.extend(fetch_biorxiv(seen_papers, limit=10))
    all_papers.extend(fetch_pubmed(seen_papers, max_results=5))
    
    log(f"\nğŸ“Š å®½å¬å›é˜¶æ®µï¼šå…±è·å– {len(all_papers)} ç¯‡å€™é€‰è®ºæ–‡...\n")
    
    if not all_papers:
        log("æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ã€‚")
        return

    report_body, kept_count = process_papers(all_papers)

    daily_report = f"# ğŸ§  Bio-AI æ¯æ—¥æƒ…æŠ¥å†³ç­– ({datetime.date.today()})\n"
    daily_report += f"> ğŸ“Š ä»Šæ—¥å¤§ç›˜ï¼šå¬å› {len(all_papers)} ç¯‡ -> AI ä¸¥é€‰ {kept_count} ç¯‡\n"
    daily_report += "> ğŸ¤– å¼•æ“ç­–ç•¥ï¼šGemini 2.5 Flash (Main) + Zhipu GLM-4 (Backup)\n\n"
    
    if kept_count == 0:
        daily_report += "### ä»Šæ—¥æ— é«˜ä»·å€¼è®ºæ–‡å…¥é€‰\nå»ºè®®æ˜å¤©ç»§ç»­å…³æ³¨ã€‚\n"
    else:
        daily_report += report_body

    print(daily_report)
    log("\nâœ… ä»»åŠ¡å®Œæˆã€‚")

if __name__ == "__main__":
    main()
